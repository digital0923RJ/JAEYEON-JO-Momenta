# JAEYEON-JO - Momenta Audio Deepfake Detection

## Summary

To briefly summarize, I selected three models‚ÄîResNet2, LCNN, and Wav2Vec2.0‚Äîfor comparison.  
After reviewing several papers, I found that most models struggle to generalize well.  
Rather than prioritizing maximum accuracy, I focused on **real-time detection** of audio deepfakes.  
If this were a competition or a task purely about performance, I would have chosen heavier models.  
However, if a model can reliably achieve ~90% accuracy, my priority is to **respond quickly** with a lightweight model in real-world scenarios.

---

## Audio Deepfake Detection Models Comparison Table

| Model         | Key Technical Innovation                                       | Reported Performance                       | Why It's Promising                                                                 | Potential Limitations                                                           |
|---------------|---------------------------------------------------------------|---------------------------------------------|--------------------------------------------------------------------------------------|----------------------------------------------------------------------------------|
| **ResNet2**   | - Residual learning<br>- Dilated convolutions & attention pooling | - EER ‚â§ **1.6%** (ADD 2022)<br>- Accuracy **98%+** | - Powerful deep feature extraction<br>- Robust across conditions<br>- Great for conversational detection | - High computational cost<br>- Needs Transformer for long-range context         |
| **LCNN**      | - Lightweight CNN<br>- Max-Feature-Map (MFM) for noise suppression | - EER ~**3‚Äì5%**<br>- Real-time capable       | - Ideal for **real-time detection**<br>- Edge deployable<br>- Noise-tolerant         | - Limited context modeling<br>- Weaker for complex, long speech segments        |
| **Wav2Vec2.0**| - Self-supervised Transformer<br>- Raw audio input             | - EER ‚â§ **2.2%**<br>- Accuracy **97%+**     | - Captures long context, emotion, and semantics<br>- Strong few-shot performance     | - Large model size<br>- High compute requirement<br>- Hard to use on edge       |

---

### ‚úÖ Why I Chose LCNN

As shown in the table below, LCNN is:
- ‚úÖ Real-time capable  
- ‚úÖ Edge-device friendly  
- ‚úÖ Compatible with log-Mel features  
- ‚úÖ Performs well with lightweight setups

| Model         | Accuracy | EER     | Real-time Capability       | Conversational Analysis     |
|---------------|----------|---------|----------------------------|-----------------------------|
| **ResNet2**   | 98%+     | ‚â§1.6%   | ‚ö†Ô∏è Moderate (needs optimization) | ‚úÖ Highly suitable           |
| **LCNN**      | ~95%     | 3‚Äì5%    | ‚úÖ Very high                | ‚ö†Ô∏è Limited                   |
| **Wav2Vec2.0**| 97%+     | ‚â§2.2%   | ‚ùå Not suitable             | ‚úÖ Best performance          |

While LCNN may have slightly lower accuracy, it‚Äôs the most practical for real-time use.  
Because LCNN lacks deep architecture, I focused on **strong preprocessing** to extract meaningful features.  
I chose `log-Mel + SpecAverage` as my primary preprocessing method, commonly used in research.  
SpecAverage helps preserve feature diversity by masking values with average energy.  
Mel spectrograms are sensitive to duration, so I normalized all inputs to **5 seconds** using padding.

---

## Part 3: Documentation & Analysis

### 1Ô∏è‚É£ Implementation Challenges

| **Challenge**                        | **Details**                                                                                                      |
|-------------------------------------|------------------------------------------------------------------------------------------------------------------|
| Large Dataset Handling              | Cached extracted log-Mel spectrograms as `.npy` files to avoid recomputation and speed up training.             |
| File Format Inconsistencies         | Used case-insensitive glob pattern `**/*.[wW][aA][vV]` to handle both `.wav` and `.WAV` files uniformly.        |
| Overfitting Risk & Duration Bias    | Trimmed/padded all audio clips to 5 seconds to remove duration-based bias.                                      |
| Model Generalization                | Used SpecAverage augmentation to help generalize to unseen fake audio.                                          |
| Lack of Real Human Speech Data      | Added LJSpeech dataset to represent genuine human voice and increase dataset realism.                           |

### 2Ô∏è‚É£ Solutions to Challenges

| **Challenge**             | **Solution**                                                                 |
|---------------------------|------------------------------------------------------------------------------|
| Large dataset             | Cached `.npy` log-Mel features for reuse                                     |
| File format issues        | Used `**/*.[wW][aA][vV]` glob pattern                                         |
| Audio length bias         | Fixed all audio to 5 seconds via padding/trimming                            |
| Overfitting               | Applied SpecAverage and selected a lightweight LCNN architecture             |
| Lack of real speech data  | Included LJSpeech dataset for authentic audio                                |

### 3Ô∏è‚É£ Assumptions Made

- **5 seconds** of audio contains sufficient signal for detecting deepfake cues.
- **log-Mel + SpecAverage** preprocessing captures richer features than MFCC alone.
- **LCNN** is sufficient for baseline detection in real-time applications.
- Case differences in file extensions (e.g., `.wav` vs `.WAV`) are syntactic, not acoustic.

---

### üìä Model Pipeline Summary

| Component          | Description                                                                 |
|--------------------|-----------------------------------------------------------------------------|
| **Preprocessing**  | Applied `log-Mel + SpecAverage` using `librosa`.                            |
| **Caching**        | Saved spectrograms as `.npy` for faster future access.                      |
| **Data Split**     | Used stratified `train_test_split` to balance labels.                       |
| **Model**          | LCNN with ReLU activations, 2 CNN layers, and AdaptiveAvgPooling.           |
| **Training/Eval**  | Used `BCELoss` and `Adam`; tracked AUC, ACC, and EER per epoch.             |
| **Visualization**  | Saved performance bar chart as `lcnn_performance.png`.                      |
| **Output**         | Printed metrics to console; exported visualization image.                   |

---

### ‚úÖ High-Level Model Description

- **Input:** Each 5-second audio clip is transformed into a log-Mel spectrogram with SpecAverage masking.
- **Architecture:** LCNN with 2 convolutional layers + ReLU + BatchNorm + AdaptiveAvgPool, followed by a fully connected layer with Sigmoid.
- **Loss:** Binary Cross-Entropy (BCELoss)
- **Metrics:**  
  - AUC: Area under the ROC curve  
  - ACC: Classification accuracy  
  - EER: Equal Error Rate ‚Äî when False Positive Rate equals False Negative Rate

---

### ‚úÖ Performance Results

- **LCNN Epoch 20:**  
  `Loss = 0.6353`  
  `AUC = 0.7233`  
  `ACC = 0.5421`  
  `EER = 0.3371`

---

### üîç Strengths & Weaknesses

#### ‚úÖ Strengths

- **Lightweight & Fast:** Suitable for edge deployment with limited resources.  
- **Effective on Short Clips:** Handles 5-second inputs well.  
- **Interpretable:** Simple architecture makes debugging and visualization easier.

#### ‚ö†Ô∏è Weaknesses

- **Overfitting Risk:** Tends to memorize small datasets.  
- **Limited Context:** LCNN lacks long-range modeling like Transformers.  
- **Performance Ceiling:** Struggles to surpass 75‚Äì80% AUC without richer features or ensembles.

---

### ‚úÖ Suggestions for Future Improvements

- Use more realistic datasets: **ASVspoof**, **FakeAVCeleb**, call center audio.
- Add data from **noisy, multilingual, and modern TTS** environments (e.g., VITS, Bark, Vall-E).
- Incorporate advanced augmentation: SpecMix, pitch shifting, time-stretching, background noise.

---

## 3Ô∏è‚É£ Reflection

### Q1 & Q2: Major Challenges & Real-World Performance

| Category                | Challenge / Consideration                                                       | Solution / Implementation                                                  |
|-------------------------|----------------------------------------------------------------------------------|-----------------------------------------------------------------------------|
| File Format Issues   | Case mismatch in `.wav` vs `.WAV` created loading bugs                          | Case-insensitive loader and file handling                                   |
| Preprocessing Bias   | Model overfitting to audio duration rather than content                         | Applied fixed-length 5s input + SpecAverage augmentation                    |
| Model Selection       | Needed a balance of speed vs performance for production                         | Chose LCNN for fast, efficient deployment                                   |
| Realism in Dataset   | Many datasets are synthetic only                                                | Included real human speech via LJSpeech dataset                             |

---

### Q3: What Additional Data Would Improve Performance?

- **Noisy real-world audio** (e.g., phone calls, YouTube, podcasts)  
- **Modern fake audio** (e.g., Vall-E, Bark, VITS samples)  
- **Multilingual datasets** to improve generalization  
- **Augmentations** like SpecMix, pitch shifting, time-stretching, noise injection

---

### Q4: Deployment Strategy

#### Dataset Expansion
- Add data from diverse sources, languages, and TTS models for generalization.

####  Cloud-Based Automation
- Use **AWS SageMaker** or **Azure ML Pipelines** to automate training and deployment workflows.
- Store and manage datasets in **S3** or **Azure Blob Storage**.

#### Inference Pipeline
- Apply 5-second **sliding window** over long audio for real-time chunk-wise prediction.
- Aggregate outputs for final decisions.

#### Edge Deployment
- Deploy **LCNN** on mobile or embedded devices for fast inference:
  - Mobile phones (iOS/Android)
  - Raspberry Pi / Jetson / IoT devices
- Convert model to **ONNX**, **TorchScript**, or **TFLite**.

#### Monitoring & Retraining
- Set up real-time dashboards (CloudWatch, Grafana)  
- Monitor prediction confidence and drift  
- Retrain on new samples as synthesis methods evolve




# JAEYEON-JO-Momenta

Í∞ÑÎã®ÌïòÍ≤å Ïö∞ÏÑ† ÏÑ§Î™ÖÏùÑ ÌïòÏûêÎßå, ÎÇòÎäî ResNet2, LCNN, Wav2Vec2.0Î•º ÏÑ†Ï†ïÌïòÏòÄÎã§. 
ÎÖºÎ¨∏ÏùÑ ÏùΩÏñ¥Î≥¥Îãà, ÎåÄÎ∂ÄÎ∂ÑÏùò Î™®Îç∏Ïù¥ ÏïÑÏßÅÍπåÏßÄÎäî ÏùºÎ∞òÌôî Ïãú ÏÑ±Îä•Ïù¥ Îñ®Ïñ¥ÏßÄÎäî Í≤ÉÏùÑ Î≥¥Í≥† ÏÑ±Îä•Ïùò Ïö∞ÏÑ†Î≥¥Îã§Îäî  real-timeÏóêÏÑú deepfakeÎ•º Ïã§ÏãúÍ∞Ñ Í∞êÏßÄÌïòÎäî Í≤ÉÏóê ÏµúÏö∞ÏÑ†ÏùÑ ÎëêÏóàÎã§. ÎßåÏïΩ Ïù¥Í≤ÉÏù¥ ÎåÄÌöåÍ±∞ÎÇò ÏÑ±Îä• Ìñ•ÏÉÅÏóêÎßå Ï¥àÏ†êÏùÑ ÎßûÏ∑ÑÏúºÎ©¥ computationÏù¥ ÎßéÏù¥ ÏöîÍµ¨ÎêòÎäî Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌñàÏùÑ Í≤ÉÏù∏Îç∞, Í∑∏Í≤å ÏïÑÎãàÎùº, Ï†ïÌôïÎèÑÍ∞Ä ÏïΩ 90%ÍπåÏßÄ ÌôïÎ≥¥Îê¨Îã§Í≥† ÏÉùÍ∞ÅÌïòÎ©¥ Í∑∏ÎÉ• Ïã§ÏãúÍ∞Ñ Ï†ïÎ≥¥Ïóê Îπ†Î•¥Í≤å ÎåÄÏùëÌï† Ïàò ÏûàÎäî Î™®Îç∏ÏùÑ ÏÑ†Ï†ïÌñàÎã§. 

## üîç Audio Deepfake Detection Models Comparison Table ÏïÑÎûòÎäî ÎÖºÎ¨∏ÏùÑ ÏùΩÍ≥† ÎÇ¥Í∞Ä 3Í∞ú ÏÑ†ÌÉùÌïú Î™®Îç∏ Î∂ÑÏÑùÏù¥Îã§. 

| Model       | Key Technical Innovation | Reported Performance | Why It's Promising | Potential Limitations |
|-------------|---------------------------|-----------------------|----------------------|------------------------|
| **ResNet2** | - Residual learning structure<br>- Can incorporate dilated convolution & attention pooling | - EER ‚â§ **1.6%** (ADD 2022)<br>- Accuracy **98%+** | - Strong deep feature extraction<br>- Robust across varied conditions<br>- Well-suited for conversation-based detection | - High computational cost<br>- Limited ability to capture long-term context (needs Transformer integration) |
| **LCNN**    | - Lightweight CNN with Max-Feature-Map (MFM)<br>- Effective noise suppression | - EER around **3‚Äì5%**<br>- Real-time capable | - Excellent for **real-time detection**<br>- Suitable for edge devices<br>- Performs well in noisy environments | - Lacks deep contextual understanding when used alone<br>- Not ideal for complex conversations |
| **Wav2Vec2.0** | - Self-supervised learning<br>- Raw audio input + Transformer-based architecture | - EER ‚â§ **2.2%**<br>- Accuracy **97%+** | - Outstanding at capturing long context, emotion, and meaning<br>- Performs well with **limited labeled data**<br>- Ideal for real conversation analysis | - Large model size and high compute cost<br>- Requires optimization for real-time deployment |

---
### üîç Í∑∏Î¶¨Í≥† ÎÇòÎäî Ïù¥ Ï§ëÏóêÏÑú LCNNÏùÑ ÏÑ†ÌÉùÌñàÎã§. Why my choice is LCNN?
ÏïÑÎûò ÌëúÏóêÏÑú Î≥¥Ïù¥ÎìØ, 
- ‚úÖ Real-time capable  
- ‚úÖ Easy to deploy on edge devices  
- ‚úÖ Compatible with log-Mel features  
- ‚úÖ Performs well with low-compute environments

| Model       | Accuracy | EER     | Real-time Capability | Conversational Analysis |
|-------------|----------|---------|------------------------|--------------------------|
| ResNet2     | 98%+     | ‚â§1.6%   | ‚ö†Ô∏è Moderate (needs optimization) | ‚úÖ Highly suitable |
| LCNN        | ~95%     | 3‚Äì5%    | ‚úÖ Very high           | ‚ö†Ô∏è Limited |
| Wav2Vec2.0  | 97%+     | ‚â§2.2%   | ‚ùå Needs high resources | ‚úÖ Best performance |



Î≥¥Î©¥ LCNNÏùÄ Ï†ïÌôïÎèÑÍ∞Ä 95%ÍπåÏßÄ Ïò¨ÎùºÍ∞ÄÍ≥† Very high real-time capacityÍ∞Ä ÏûàÎã§Í≥† ÏÉùÍ∞ÅÎêòÏñ¥ LCNN Î™®Îç∏ÏùÑ ÏÑ†ÌÉùÌñàÎã§. 
ÎåÄÏã†, Ï†ÑÏ≤òÎ¶¨Í∞Ä Ï§ëÏöîÌïòÏó¨ Ï†ÑÏ≤òÎ¶¨Î•º ÏûòÌïòÎ†§Í≥† ÎÖ∏Î†•ÌñàÎã§.
ÎåÄÎûµÏ†ÅÏúºÎ°ú ÌÅ¨Í≤å Ï†ÑÏ≤òÎ¶¨ÌïòÎäî Î∞©ÏãùÏùÄ MFCC, Mel, SpecAverageÍ∞Ä ÎÖºÎ¨∏Î≥¥ÎãàÍπå ÎßéÏù¥ Ïì∞ÏòÄÎçîÎùº. Í∑∏ÎûòÏÑú ÎÇòÎèÑ Ïù¥ Î∞©Î≤ïÏùÑ ÌÜµÌï¥ Ï†ÑÏ≤òÎ¶¨ ÌïòÎ†§Í≥† ÌñàÎã§. 
LCNNÏùÄ Ï†ïÌôïÎèÑÍ∞Ä ÏÉÅÎåÄÏ†ÅÏúºÎ°ú ÎÇÆÏùÄ Î™®Îç∏Ïù¥Îùº Ï†ÑÏ≤òÎ¶¨Î•º ÌÜµÌï¥ ÏµúÎåÄÌïú ÎßéÏùÄ feature ÌäπÏßïÎì§ÏùÑ ÎΩëÏúºÎ†§Í≥† ÌñàÎã§. 
Í∑∏ÎûòÏÑú 1Î≤àÏùò Ï†ÑÏ≤òÎ¶¨Î≥∏Îã§Îäî log-Mel + SpecAvaerge Ï†ÑÏ≤òÎ¶¨Î•º ÏÑ†ÌÉùÌñàÎã§. SpecAverageÎ•º ÏÑ†ÌÉùÌïú Í≤ÉÏùÄ ÎßàÏä§ÌÇπÏùÑ ÌèâÍ∑†ÏúºÎ°ú Ï°∞Ï†àÌïòÎ©¥ÏÑú ÌäπÏÑ±ÏùÑ Ïûò ÎΩëÏùÑ Ïàò ÏûàÍ∏∞Ïóê ÏÑ†ÌÉùÌñàÎã§. 
ÎåÄÏã† MelÏùÄ Ïò§ÎîîÏò§ Í∏∏Ïù¥Ïóê ÎØºÍ∞êÌïòÍ∏∞Ïóê Î™®Îì† Ïò§ÎîîÏò§Î•º 5Ï¥àÎ°ú ÌÜµÏùºÌïòÎ†§Í≥† ÌñàÍ≥† paddingÌï® 


## Part 3: Documentation & Analysis

1. Document your implementation process, including:
    - Any challenges encountered

| **Challenge**                       | **Details**                                                                                                       |
|------------------------------------|-------------------------------------------------------------------------------------------------------------------|
| **Large Dataset Handling**         | Cached extracted log-Mel spectrograms as `.npy` files to avoid recomputation and reduce training time.           |
| **File Format Inconsistencies**    | Used case-insensitive glob pattern `**/*.[wW][aA][vV]` to handle `.wav` and `.WAV` files uniformly.               |
| **Overfitting Risk & Duration Bias** | Applied fixed-length preprocessing: all audio clips were trimmed or zero-padded to 5 seconds to remove bias.     |
| **Model Generalization**           | Used SpecAverage augmentation to reduce overfitting and promote robustness in learning.                          |
| **Lack of Real Human Speech Data** | Added the LJSpeech dataset to introduce authentic human voice samples and improve realism.                       |
      
    - How you addressed these challenges
 | **Challenge**            | **Solution**                                                                 |
|--------------------------|------------------------------------------------------------------------------|
| Large dataset            | Used **caching (`.npy`)** of extracted log-Mel features to avoid recomputation |
| File format issues       | Implemented case-insensitive glob pattern: `**/*.[wW][aA][vV]`                |
| Audio length bias        | All audio was **padded or trimmed to 5 seconds** using fixed-length preprocessing |
| Overfitting              | Applied **SpecAverage augmentation** and used a **lightweight CNN model (LCNN)** |
| Real speech data scarcity | Added **LJSpeech dataset** to represent real human voice                    |
   

    - Assumptions made
    - **5 seconds of audio** is enough to capture the key acoustic features necessary to distinguish between real and fake speech.
- Using **log-Mel spectrograms combined with SpecAverage masking** improves generalization and reduces overfitting compared to MFCCs alone.
- A **lightweight architecture (LCNN)** is sufficient for baseline detection without requiring large-scale Transformer-based models like Wav2Vec 2.0.
- Both `.wav` and `.WAV` formats are **acoustically identical**; the issue is purely syntactic and handled during file loading.

2. Include an analysis section that addresses:
üìä Model Analysis
| Íµ¨ÏÑ± ÏöîÏÜå     | ÏÑ§Î™Ö |
|----------------|------|
| **Ï†ÑÏ≤òÎ¶¨**      | `log-Mel + SpecAverage` Ï†ÅÏö©. `librosa`Î•º Ïù¥Ïö©ÌïòÏó¨ Ïò§ÎîîÏò§Î•º Î°úÎî©ÌïòÍ≥† Ïä§ÌéôÌä∏Î°úÍ∑∏Îû® Î≥ÄÌôò ÏàòÌñâ. |
| **Ï∫êÏã±**        | `.npy` ÌòïÌÉúÎ°ú Ï†ÄÏû•ÌïòÏó¨ Ïû¨ÏÇ¨Ïö© Í∞ÄÎä•, Îπ†Î•∏ Îç∞Ïù¥ÌÑ∞ Î°úÎî© Í∞ÄÎä•. |
| **Îç∞Ïù¥ÌÑ∞ Î∂ÑÎ¶¨** | `train_test_split`Ïóê Í∏∞Î∞òÌïòÏó¨ stratified split Ï†ÅÏö©, ÌÅ¥ÎûòÏä§ ÎπÑÏú® Ïú†ÏßÄ. |
| **Î™®Îç∏ Íµ¨Ï°∞**   | LCNN Í∏∞Î∞ò Íµ¨Ï°∞. `ReLU` ÌôúÏÑ±Ìôî Ìï®Ïàò ÏÇ¨Ïö©. 2Ï∏µ CNN + `AdaptiveAvgPool`Î°ú Ï∂úÎ†• Ï†ïÍ∑úÌôî. |
| **ÌõàÎ†®/ÌèâÍ∞Ä Î£®ÌîÑ** | `BCELoss`ÏôÄ `Adam` Optimizer ÏÇ¨Ïö©. Îß§ epochÎßàÎã§ `AUC`, `ACC`, `EER` ÏßÄÌëú Ï∂úÎ†•. |
| **ÏãúÍ∞ÅÌôî**     | ÏµúÏ¢Ö ÏÑ±Îä•ÏùÑ `ÎßâÎåÄÍ∑∏ÎûòÌîÑ`Î°ú ÏãúÍ∞ÅÌôîÌïòÏó¨ `lcnn_performance.png`Î°ú Ï†ÄÏû•. |
| **ÏµúÏ¢Ö Ï∂úÎ†•**   | ÏΩòÏÜîÏóê ÌèâÍ∞Ä ÏßÄÌëú Ï∂úÎ†• + Ïù¥ÎØ∏ÏßÄ ÌååÏùº Ï†ÄÏû• (`lcnn_performance.png`). |

### ‚úÖ Why LCNN Was Selected

- **Real-time capability:**  
  LCNN (Lightweight Convolutional Neural Network) is fast and computationally efficient‚Äîideal for edge or mobile deployment.

- **Proven effectiveness:**  
  LCNN has been shown to perform well in fake audio detection tasks with relatively low error rates.

- **Compatibility:**  
  Works well with log-Mel spectrograms and benefits from data augmentations like SpecAverage.

- **Interpretability:**  
  Easier to visualize and understand than Transformer-based models.

### ‚úÖ How the model works (high-level technical explanation)

- **Input:** Each audio file is converted into a log-Mel spectrogram, augmented using SpecAverage masking, and reshaped to a fixed length (5 seconds)
- **Model Architecture:** Two convolutional layers with ReLU, followed by BatchNorm and Adaptive Average Pooling.Fully connected layer with a sigmoid activation outputs the probability of the audio being fake.
- **Loss Function:** Binary Cross-Entropy Loss (BCELoss) is used since this is a binary classification problem (real vs. fake).
- **Evaluation Metrics:** AUC (Area Under ROC Curve): How well the model separates the two classes, Accuracy: Proportion of correct predictions, EER (Equal Error Rate): The point where False Positive Rate = False Negative Rate.

### ‚úÖ Performance results on your chosen dataset
- #### LCNN Epoch 20: Loss=0.6353, AUC=0.7233, ACC=0.5421, EER=0.3371
  
### üîç Strengths & Weaknesses
#### üü¢ Strengths

- **Lightweight & Fast**  
  Can be deployed in low-resource environments.

- **Effective on Short Clips**  
  Works reasonably well with 5-second samples.

- **Interpretable**  
  Simpler architecture makes debugging and tuning easier.

#### üî¥ Weaknesses

- **Overfitting**  
  Model quickly memorizes small datasets.

- **Limited Context**  
  LCNN lacks long-range temporal modeling like Transformers.

- **Performance Ceiling**  
  Struggles to exceed 75‚Äì80% AUC without ensemble or richer features.

### ‚úÖ Suggestions for future improvements
- Improve Dataset Quality & Variety:
  Use ASVspoof, FakeAVCeleb, and call center-style datasets for better generalization.
  Add noisy environments, different languages, and newer fake voice synthesis methods (VITS, Bark, Vall-E).


## 3. Reflection questions to address:

### Q1: What were the most significant challenges? & Q2: How might this approach perform in the real world?

| Category                | Challenge / Consideration                                                     | Solution / Implementation                                                                 |
|-------------------------|-------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|
| üîÑ File Format Issues   | Large dataset size, case-sensitive `.wav` vs `.WAV` caused read errors       | Used consistent loader logic and added compatibility for both `.wav` and `.WAV` extensions  |
| ‚öôÔ∏è Preprocessing Dilemma | Model overfitting, generalization problems, bias due to length differences   | Applied `log-Mel + SpecAverage`, and fixed all audio to **5 seconds** using pad/crop       |
| ‚öñÔ∏è Model Selection       | Trade-off between inference speed and accuracy                               | Chose **LCNN** for fast, real-time capable, lightweight architecture                        |
| üîç Real Data Requirement | Needed realistic audio (not just synthetic/generated)                        | Added **LJSpeech** dataset as genuine human voice source                                    |
---

### Q3: What additional data would improve performance?

- **Noisy and real-life audio**  
  e.g., phone calls, YouTube recordings, podcast snippets.

- **Fake audio from modern generators**  
  Incorporate samples from models like **Vall-E**, **Bark**, or **VITS** to stay up to date with the latest synthesis techniques.

- **Multilingual data**  
  To ensure the model generalizes across different languages and accents.

- **Advanced data augmentation techniques**  
  Such as: SpecMix, Pitch shift, Time-stretching, Background noise injection
---

### Q4: How would you deploy this?

#### üì¶ Dataset Expansion for Generalization
- **Add more diverse data** to improve model generalization:
  - Noisy real-world audio (e.g., phone calls, podcasts, YouTube)
  - Multiple languages and accents
  - Fake audio from modern TTS models (e.g., Vall-E, Bark, VITS)
- Ensure dataset includes varied recording environments, devices, and speakers.

#### ü§ñ Model Training Automation (Cloud)
- Use **cloud platforms like AWS or Azure** to automate model training, evaluation, and deployment:
  - Leverage **AWS SageMaker** or **Azure ML Pipelines**

#### üß† Inference Pipeline
- Implement **batch inference** for stored datasets and **real-time streaming inference** for live applications.

#### üì± Edge Deployment
- Use the **LCNN model** for **low-latency, lightweight inference** on edge devices:
  - Mobile phones (iOS/Android)
  - Embedded systems (Raspberry Pi, NVIDIA Jetson)
  - IoT devices (smart assistants, surveillance)

- Convert model to **ONNX**, **TorchScript**, or **TFLite** for optimal performance on various platforms.

#### üìä Monitoring & Retraining
- **Continuously monitor**:
  - Prediction confidence levels
  - Accuracy drift over time
  - New types of fake audio
- Set up **automated retraining pipelines** triggered by performance degradation or new data.
- Use dashboards to visualize performance (e.g., via Amazon CloudWatch, Azure Monitor, or Grafana).
---
























## üìö Appendix

### üìÅ Dataset Info

- **Real**: LJSpeech (~13,000 clips, ~10 seconds each)  
- **Fake**: MelGAN-generated audio (~5 seconds)  
- ‚úÖ Class-balanced cache system implemented

---

### üß™ Training Pipeline Highlights

- `extract_logmel()` ensures uniform audio duration  
- Padding applied to ensure consistent input dimensions  
- `pad_collate_fn()` used for batch-wise padding  
- Still requires **early stopping or regularization**

---

## ‚úÖ Summary Table

| Feature                | LCNN          | ResNet2        | Wav2Vec2.0     |
|------------------------|---------------|----------------|----------------|
| Inference Speed        | ‚ö° Very fast   | ‚ö†Ô∏è Moderate     | üê¢ Very slow    |
| Accuracy (Expected)    | üëç ~95%        | ‚úÖ 98%+         | ‚úÖ 97%+         |
| EER (Expected)         | ‚ö†Ô∏è 3‚Äì5%        | ‚úÖ ‚â§1.6%        | ‚úÖ ‚â§2.2%        |
| Suitable for Streaming | ‚úÖ Yes         | ‚ö†Ô∏è With tuning  | ‚ùå Not ideal    |

---

## üöÄ Future Work

- ‚úÖ Integrate **multi-model ensemble** (LCNN + ResNet2 + distilled Wav2Vec2.0)  
- ‚úÖ Fine-tune on **ASVspoof 2021**, **FakeAVCeleb**, or more recent datasets  
- ‚úÖ Experiment with **attention pooling**, **SpecMix**, **SpecAugment++**  
- ‚úÖ Build a **frontend dashboard** for live detection and visualization

---





# üéß Audio Deepfake Detection using log-Mel + SpecAverage + LCNN

> Lightweight audio deepfake detection with strong real-time capability and generalization focus.  
> Explored multiple models and preprocessing methods.  
> Final implementation uses LCNN with SpecAverage-augmented log-Mel features.

---

## üìå Overview

This project focuses on building an efficient and practical pipeline for detecting AI-generated human speech using lightweight CNN-based models. The goal is real-time applicability while maintaining robust detection capability.

---

## 1. ‚öôÔ∏è Implementation Documentation

### ‚úÖ Challenges Encountered

1. **Large Fake Dataset**
   - Slow preprocessing and loading due to dataset size
   - Limited generalization: all audio was generated
   - Real vs. fake audio had different durations (10s vs. 5s)

2. **Wav2Vec2.0 Inference Cost**
   - Too large for real-time or mobile deployment
   - Slower than lightweight CNNs like LCNN and ResNet2

3. **File Extension Issues**
   - `.wav` vs `.WAV` (case-sensitive) caused loading failures

4. **Overfitting Risk**
   - Small dataset + deep CNNs = higher chance of memorization

5. **Preprocessing Dilemma**
   - MFCC compresses too much
   - Log-Mel is better but vulnerable to overfitting
   - SpecAverage helps with augmentation but needs tuning

6. **Length-Based Bias**
   - Model might learn to classify based on input length (real=10s, fake=5s)

---

### üõ†Ô∏è How These Were Addressed

- ‚úÖ Added real audio: **LJSpeech** dataset
- ‚úÖ Switched to lightweight models: **LCNN**, **ResNet2**
- ‚úÖ Preprocessing: used **log-Mel + SpecAverage**
- ‚úÖ Fixed-length audio: trimmed/padded all to **5 seconds**
- ‚úÖ Created custom `extract_logmel()` with duration constraint

```python
def extract_logmel(path, sr=16000, n_mels=40, duration=5.0):
    y, _ = librosa.load(path, sr=sr)
    target_len = int(sr * duration)
    y = np.pad(y, (0, max(0, target_len - len(y))))[:target_len]
    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)
    logmel = librosa.power_to_db(mel)
    return torch.tensor(spec_average_augment(logmel)).float().unsqueeze(0)


# üßæ Part 3: Documentation & Analysis

## 1. Implementation Documentation

### ‚úÖ Challenges Encountered

1. **Îç∞Ïù¥ÌÑ∞ÏÖãÏùò ÌÅ¨Í∏∞ Î¨∏Ï†ú**  
   Îã§Ïö¥Î°úÎìúÌïú fake Ïò§ÎîîÏò§ Îç∞Ïù¥ÌÑ∞ÏÖãÏù¥ ÎÑàÎ¨¥ Ïª§ÏÑú Ï†ÑÏ≤òÎ¶¨ÏôÄ Î°úÎî©Ïóê Ïò§Îûú ÏãúÍ∞ÑÏù¥ Í±∏Î†∏Í≥†, Î™®Îëê generated audioÎùºÏÑú ÏùºÎ∞òÌôîÏóê Ï†úÌïúÏù¥ ÏûàÏùÑ Ïàò ÏûàÏóàÏäµÎãàÎã§.
   Ïã§Ï†úÎ°ú Î™®Îç∏ÏùÑ ÎèåÎ†§Î≥º ÏãúÍ∞ÑÏù¥ ÎßéÏù¥ ÏóÜÏóàÏùå. 

2. **Wav2Vec 2.0 Î™®Îç∏Ïùò Ïã§ÏãúÍ∞Ñ Ï≤òÎ¶¨ ÌïúÍ≥Ñ**  
   Wav2Vec2.0ÏùÄ powerfulÌïòÍ∏¥ ÌïòÏßÄÎßå, ÌÅ¨Í∏∞Í∞Ä Ïª§ÏÑú real-time Ï†ÅÏö©ÏóêÎäî Î∂ÄÏ†ÅÌï©ÌñàÏäµÎãàÎã§. ÌäπÌûà Î™®Î∞îÏùº/Í≤ΩÎüâÌôîÍ∞Ä Ï§ëÏöîÌïú ÌôòÍ≤ΩÏóêÏÑúÎäî ÏÇ¨Ïö©Ïù¥ Ïñ¥Î†§ÏõÄ.

3. Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨: Í≥ºÏ†úÎ•º ÏúÑÌïú ÎÖºÎ¨∏Îì§ÏóêÏÑú ÎåÄÎ∂ÄÎ∂ÑÏùò audio deepfake detection modelÏùò ÌïúÍ≥ÑÏ†êÏùÑ ÏùºÎ∞òÌôîÎùºÍ≥† Ìï®. Ï¶â,
   Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨Î•º Ïñ¥ÎñªÍ≤å Ìï¥Ïïº ÏùºÎ∞òÌôî Ïãú Îñ®Ïñ¥ÏßÄÎäî ÏÑ±Îä•ÏùÑ ÏµúÎåÄÌïú Î∞©ÏßÄÌï† Ïàò ÏûàÏùÑÍπå Í≥†ÎØºÌï®. NoiseÍ∞Ä ÏóÜÎäî Í≥†ÏÑ±Îä• ÌååÏùºÎßêÍ≥† noiseÍ∞Ä ÏûàÎäî ÌååÏùº, Í∑∏Î¶¨Í≥† Îòê ÏµúÏã†Ïùò Í∏∞Ïà†Î°ú ÏÉùÏÑ±Îêú deepfake audio Îì± Îã§ÏñëÌïú Îç∞Ïù¥ÌÑ∞ÏÖã ÌôïÎ≥¥Í∞Ä ÌïÑÏöîÌï® 
   MFCC:
   Mel:
   SpecAverage:  ÏúÑÏùò 3Í∞ÄÏßÄ Ï†ÑÏ≤òÎ¶¨ Î∞©Î≤ïÏúºÎ°úÎäî Î™®Îì† deepfake audio detectionÏóê ÌïúÍ≥ÑÍ∞Ä ÏûàÏùå. 
 
4. ÏùºÎ∞ò Î®∏Ïã†Îü¨Îãù Î™®Îç∏ ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÍ∏∞. 
   ÏùºÎ∞ò Î®∏Ïã†Îü¨Îãù Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÎãàÍπå ÌôúÏÑ±Ìôî Ìï®ÏàòÍ∞Ä gradientÎ•º 0ÏúºÎ°ú ÎßåÎì¨. Ï¶â, ÍπäÏù¥ ÏûàÎäî Ïã†Í≤ΩÎßùÏùÑ Í±¥Ï∂ïÌï† ÏàòÍ∞Ä ÏóÜÏùå. 
---

### ‚úÖ How Challenges Were Addressed

1. **Ïã§Ï†ú(real) Ïò§ÎîîÏò§ Îç∞Ïù¥ÌÑ∞ÏÖã Ï∂îÍ∞Ä ÌôïÎ≥¥**  
   LJSpeech Îç∞Ïù¥ÌÑ∞ÏÖã(`"/content/drive/MyDrive/Internship /LJSpeech-1.1/wavs"`)ÏùÑ ÏÇ¨Ïö©Ìï¥ ÏùºÎ∞ò ÏùåÏÑ±Í≥º ÎπÑÍµê Í∞ÄÎä•ÌïòÎèÑÎ°ù Íµ¨ÏÑ±ÌñàÏäµÎãàÎã§.

2. **Wav2Vec 2.0 Ï†úÍ±∞**  
   Ï¥àÍ∏∞ÏóêÎäî Wav2Vec2.0ÏùÑ transfer learning ÌòïÌÉúÎ°ú feature extractorÎ°ú ÏÇ¨Ïö©ÌïòÎ†§ ÌñàÏßÄÎßå, Î™®Îç∏Ïù¥ ÎÑàÎ¨¥ Ïª§ÏÑú ÏÜçÎèÑÏôÄ Ïã§Ïö©ÏÑ± Î©¥ÏóêÏÑú Î∂ÄÏ†ÅÌï©ÌïòÎã§Í≥† ÌåêÎã®.  
   **Ï†ïÌôïÎèÑ Ï∞®Ïù¥Í∞Ä ÌÅ¨ÏßÄ ÏïäÎã§Î©¥, Ïã§Ïö©Ï†ÅÏù¥Í≥† Îπ†Î•∏ Î™®Îç∏Ïù¥ ÏÉÅÏö©ÌôîÏóê Îçî Ï†ÅÌï©**ÌïòÎã§Í≥† ÏÉùÍ∞ÅÌïòÏó¨ LCNNÍ≥º ResNetV2Î•º Ï§ëÏã¨ÏúºÎ°ú ÎπÑÍµê Ïã§ÌóòÏùÑ ÏßÑÌñâÌñàÏäµÎãàÎã§.

3. **Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨**
   ÎßéÏùÄ Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ Í∏∞Î≤ïÏù¥ ÎÖºÎ¨∏Ïóê ÏûàÏóàÎã§. MFCC, Log-Mel, SpecAverage. Ïó¨Í∏∞ÏÑú Í∞ÄÏû• Ï†ïÌôïÌïú Ï†ÑÏ≤òÎ¶¨ Î∞©Î≤ïÏùÑ Ï∞æÏïÑÏïº ÌñàÏóàÎã§.
   ÏôúÎÉêÎ©¥ Ïù¥ÎØ∏ Wav2Vec2.0Î•º Ï†úÍ±∞ ÌñàÍ∏∞Ïóê Í∞ÄÎä•Ìïú ÎßéÏùÄ feature extractionÏùÑ ÌñàÏñ¥Ïïº ÌñàÎã§.  
   Í∑∏Î¶¨Í≥† 
   - MFCC
   - Log-Mel
   - SpecAverage
  
4. Îî•Îü¨Îãù ÏÇ¨Ïö©(light version)
   ÍπäÏù¥ ÏûàÎäî Ïã†Í≤ΩÎßùÏù¥ ÏûàÏùÑ ÏàòÎ°ù ÌïôÏäµÏùÑ Îçî ÏûòÌï† Ïàò ÏûàÍ∏∞Ïóê Ïù¥Î•º Ïûò Ìï† Ïàò ÏûàÍ∏∞Ïóê LCNN ÏÇ¨Ïö©Ìï®.
   
---

## Part 3: Documentation & Analysis

1. Document your implementation process, including:
    - Any challenges encountered:
   1. Îã§Ïö¥Î°úÎìúÌïú fake Ïò§ÎîîÏò§ Îç∞Ïù¥ÌÑ∞ÏÖãÏù¥ ÎÑàÎ¨¥ Ïª§ÏÑú Ï†ÑÏ≤òÎ¶¨ÏôÄ Î°úÎî©Ïóê Ïò§Îûú ÏãúÍ∞ÑÏù¥ Í±∏Î†∏Í≥†, Î™®Îëê generated audioÎùºÏÑú ÏùºÎ∞òÌôîÏóê Ï†úÌïúÏù¥ ÏûàÏùÑ Ïàò ÏûàÏóàÏäµÎãàÎã§.
   2. Wav2Vec2.0ÏùÄ powerfulÌïòÍ∏¥ ÌïòÏßÄÎßå, ÌÅ¨Í∏∞Í∞Ä Ïª§ÏÑú real-time Ï†ÅÏö©ÏóêÎäî Î∂ÄÏ†ÅÌï©ÌñàÏäµÎãàÎã§. ÌäπÌûà Î™®Î∞îÏùº/Í≤ΩÎüâÌôîÍ∞Ä Ï§ëÏöîÌïú ÌôòÍ≤ΩÏóêÏÑúÎäî ÏÇ¨Ïö©Ïù¥ Ïñ¥Î†§ÏõÄ.
  
    - How you addressed these challenges:
    - 1. Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ ÏÖãÏùÑ Íµ¨ÌñàÎã§ - ("/content/drive/MyDrive/Internship /LJSpeech-1.1/wavs")ÏΩîÎìúÏóê ÏûàÏùå),
    - 2. Ï¥àÍ∏∞ÏóêÎäî Wav2Vec2.0ÏùÑ transfer learning ÌòïÌÉúÎ°ú feature extractorÎ°ú ÏÇ¨Ïö©ÌïòÎ†§ ÌñàÏßÄÎßå, Î™®Îç∏Ïù¥ ÎÑàÎ¨¥ Ïª§ÏÑú ÏÜçÎèÑÏôÄ Ïã§Ïö©ÏÑ± Î©¥ÏóêÏÑú Î∂ÄÏ†ÅÌï©ÌïòÎã§Í≥† ÌåêÎã®.
Ï†ïÌôïÎèÑ Ï∞®Ïù¥Í∞Ä ÌÅ¨ÏßÄ ÏïäÎã§Î©¥, Ïã§Ïö©Ï†ÅÏù¥Í≥† Îπ†Î•∏ Î™®Îç∏Ïù¥ ÏÉÅÏö©ÌôîÏóê Îçî Ï†ÅÌï©ÌïòÎã§Í≥† ÏÉùÍ∞ÅÌïòÏó¨ LCNNÍ≥º ResNetV2Î•º Ï§ëÏã¨ÏúºÎ°ú ÎπÑÍµê Ïã§ÌóòÏùÑ ÏßÑÌñâÌñàÏäµÎãàÎã§
   - LCNN ÏûÖÎ†•Ìï† Îïå, featureÏùò ÌÅ¨Í∏∞Í∞Ä Îã§Î¶Ñ.
   - ÏÉàÎ°ú ÏûÖÎ†•Ìïú Îç∞Ïù¥ÌÑ∞ÏÖã Ïò§ÎîîÏò§ Í∏∏Ïù¥Í∞Ä Í∏∏ÏóàÏùå.  
    - Assumptions made:  
  
3. Include an analysis section that addresses:
    - Why you selected this particular model for implementation: Í≤∞Í≥ºÎ•º Î≥¥Îãà ÏïôÏÉÅÎ∏îÏù¥ Ï¢ãÎã§Îäî Ìö®Í≥ºÎ•º Î¥§Îã§
    - How the model works (high-level technical explanation): SpecAverage Ï†ÑÏ≤òÎ¶¨ÌïòÍ≥† logMELÏùÑ Ìï®. ÏõêÎûòÎäî MFCCÎ•º ÌïòÎ†§Í≥† ÌñàÎäîÎç∞, Í∑ºÎç∞ ÎÑà Í∞ôÏúºÎ©¥ Ïôú SpecAverge+logMelÏùÑ ÏÑ†ÌÉùÌï¥? over MFCC? 
    - Performance results on your chosen dataset: ÏïÑÏßÅ Î™®Îç∏ ÎèåÎ¶¨Îäî Ï§ë 
    - Observed strengths and weaknesses: 
    - Suggestions for future improvements: 

4. Reflection questions to address:
    1. What were the most significant challenges in implementing this model? 
    2. How might this approach perform in real-world conditions vs. research datasets?
    3. What additional data or resources would improve performance?
    4. How would you approach deploying this model in a production environment?
  
   ÌôïÏû•Ïûê „ÖÖ„ÖÇ....wav. WAV Ï∞®Ïù¥Í∞Ä ÏûàÏùÑÏßÄ...

   üéØ Î¨∏Ï†ú Îã§Ïãú Ï†ïÎ¶¨:
real Ïò§ÎîîÏò§Îäî ÏïΩ 10Ï¥à, fake Ïò§ÎîîÏò§Îäî ÏïΩ 5Ï¥à
‚Üí Î™®Îç∏Ïù¥ "Í∏∏Ïù¥ Ï∞®Ïù¥"ÎßåÏúºÎ°ú ÏßÑÏßú/Í∞ÄÏßúÎ•º Î∂ÑÎ•òÌï† ÏúÑÌóò
‚Üí ÏùºÎ∞òÌôî ÏÑ±Îä• Ï†ÄÌïò Í∞ÄÎä•

def extract_logmel(path, sr=16000, n_mels=N_MELS):
    y, _ = librosa.load(path, sr=sr)
    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)
    ...
    Îî∞ÎùºÏÑú log-MelÎèÑ ÏãúÍ∞Ñ Ï∂ï(t) Í∏∏Ïù¥Í∞Ä ÏÑúÎ°ú Îã¨ÎùºÏßëÎãàÎã§.
    Ìå®Îî©: ÏßßÏùÄ Ïò§ÎîîÏò§ ÌååÏùº Îí§Ïóê 0 Í∞íÏùÑ Ï±ÑÏõå Í∏∏Ïù¥Î•º ÎßûÏ∂îÎäî Î∞©Î≤ï
    Î¨∏Ï†ú ÏûàÏùå: librosa.load()Îäî Ïò§ÎîîÏò§ Í∏∏Ïù¥Ïóê Îî∞Îùº Í∑∏ÎåÄÎ°ú Î°úÎî©Ìï©ÎãàÎã§.

Ï¶â, realÏùÄ 10Ï¥à ‚Üí 160,000 samples, fakeÎäî 5Ï¥à ‚Üí 80,000 samples

Îî∞ÎùºÏÑú log-MelÎèÑ ÏãúÍ∞Ñ Ï∂ï(t) Í∏∏Ïù¥Í∞Ä ÏÑúÎ°ú Îã¨ÎùºÏßëÎãàÎã§.

def pad_collate_fn(batch):
    features, labels = zip(*batch)
    max_len = max([f.shape[-1] for f in features])
    padded = [torch.nn.functional.pad(f, (0, max_len - f.shape[-1])) for f in features]
    return torch.stack(padded), torch.tensor(labels).float()
Î∂ÄÎ∂ÑÏ†ÅÏúºÎ°ú Ìï¥Í≤∞Îê®: Î∞∞Ïπò Îã®ÏúÑÏóêÏÑúÎäî spectrogramÏùò Í∏∏Ïù¥Î•º Ìå®Îî©ÏúºÎ°ú ÎßûÏ∂îÍ≥† ÏûàÏùå

ÌïòÏßÄÎßå ÏõêÎûò Í∏∏Ïù¥Ïóê Îî∞Î•∏ Ï†ïÎ≥¥Îäî Ïó¨Ï†ÑÌûà featureÏóê ÎÇ®ÏïÑ ÏûàÏùå

Ï¶â, Î™®Îç∏Ïù¥ Ïó¨Ï†ÑÌûà **"ÏßßÏùÄ Ïä§ÌéôÌä∏Î°úÍ∑∏Îû®Ïù¥Î©¥ fake, Í∏∏Î©¥ real"**Ïù¥ÎùºÎäî ÌûåÌä∏Î•º ÌïôÏäµÌï† Í∞ÄÎä•ÏÑ± ÏûàÏùå
‚Üí ÌïôÏäµÏù¥ Í∏∏Ïù¥Ïóê biasÎê† Ïàò ÏûàÍ≥†, Ïù¥Îäî generalizationÏóê Ï∑®ÏïΩÏ†êÏù¥ Îê©ÎãàÎã§.


Î™®Îì† Ïò§ÎîîÏò§Î•º 5Ï¥àÎ°ú ÏûòÎùºÏÑú Í≥†Ï†ï ‚Üí Í∏∏Ïù¥ Ï†ïÎ≥¥Í∞Ä Î™®Îç∏Ïóê Î∞òÏòÅÎêòÏßÄ ÏïäÏùå

ÏßÑÏßú/Í∞ÄÏßú Î™®Îëê Í∞ôÏùÄ Í∏∏Ïù¥Î°ú ÎπÑÍµêÎêòÎØÄÎ°ú Í∏∏Ïù¥Ïóê ÏùòÏ°¥Ìïú Î∂ÑÎ•ò Í∞ÄÎä•ÏÑ± Ï∞®Îã®
‚úÖ Í≤∞Î°†
Ìï≠Î™©	ÌòÑÏû¨ ÏΩîÎìú	Ìï¥Í≤∞ Ïó¨Î∂Ä
Ïò§ÎîîÏò§ Í∏∏Ïù¥ ÌÜµÏùº	‚ùå (librosa.load()Î°ú ÏõêÎ≥∏ Í∏∏Ïù¥ Ïú†ÏßÄ)	Ìï¥Í≤∞ Ïïà Îê®
Í∏∏Ïù¥ ÎßûÏ∂§ Ï≤òÎ¶¨	‚≠ï (pad_collate_fnÏúºÎ°ú Ìå®Îî©)	ÏùºÎ∂Ä Ìï¥Í≤∞ (Î™®Ïñë ÎßûÏ∂§Îßå)
Í∏∏Ïù¥ bias Ï†úÍ±∞	‚ùå Î™®Îç∏Ïù¥ Ïó¨Ï†ÑÌûà Í∏∏Ïù¥ Ï†ïÎ≥¥Ïóê Ï†ëÍ∑º Í∞ÄÎä•	ÏàòÏ†ï ÌïÑÏöî


‚úÖ ÏàòÏ†ïÎêú extract_logmel() Ìï®Ïàò
python
Î≥µÏÇ¨
Ìé∏Ïßë
def extract_logmel(path, sr=16000, n_mels=N_MELS, duration=5.0):
    y, _ = librosa.load(path, sr=sr)

    # Í∏∏Ïù¥Î•º 5Ï¥àÎ°ú Í≥†Ï†ï (Î∂ÄÏ°±ÌïòÎ©¥ 0-padding, ÎÑòÏπòÎ©¥ ÏûêÎ•¥Í∏∞)
    target_length = int(sr * duration)
    if len(y) < target_length:
        y = np.pad(y, (0, target_length - len(y)))
    else:
        y = y[:target_length]

    # Mel ‚Üí log-Mel Î≥ÄÌôò
    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)
    log_mel = librosa.power_to_db(mel)

    # SpecAverage augmentation
    log_mel_aug = spec_average_augment(log_mel)

    return torch.tensor(log_mel_aug).float()

    üîß Ï†ÅÏö© Î∞©Î≤ï
Í∏∞Ï°¥ ÏΩîÎìúÏóêÏÑú extract_logmel() Ìï®Ïàò Ï†ÑÏ≤¥Î•º ÏúÑ ÎÇ¥Ïö©ÏúºÎ°ú ÍµêÏ≤¥ÌïòÏÑ∏Ïöî.

Ìò∏Ï∂úÌïòÎäî Ï™Ω(Ïòà: __getitem__)ÏùÄ Í∑∏ÎåÄÎ°ú Ïú†ÏßÄÌï¥ÎèÑ ÏûëÎèôÌï©ÎãàÎã§.

üìå Ïù¥Î°ú Ïù∏Ìï¥ ÏñªÎäî Ìö®Í≥º
Î≥ÄÍ≤Ω Ï†Ñ	Î≥ÄÍ≤Ω ÌõÑ
Ïò§ÎîîÏò§ Í∏∏Ïù¥Í∞Ä Ï†úÍ∞ÅÍ∞Å ‚Üí ÏãúÍ∞ÑÏ∂ï Í∏∏Ïù¥ Îã§Î¶Ñ ‚Üí Î™®Îç∏Ïù¥ Í∏∏Ïù¥Î°ú ÌïôÏäµ Í∞ÄÎä•ÏÑ±	Ïò§ÎîîÏò§ Í∏∏Ïù¥Î•º 5Ï¥àÎ°ú Í≥†Ï†ï ‚Üí ÏãúÍ∞ÑÏ∂ïÎèÑ ÏùºÏ†ï ‚Üí Î™®Îç∏Ïù¥ Í∏∏Ïù¥Î°ú ÌåêÎã® Î∂àÍ∞Ä
fakeÎäî 5Ï¥à, realÏùÄ 10Ï¥à ‚Üí Î¨¥ÏùòÏãùÏ†Å bias	ÎèôÏùºÌïú Í∏∏Ïù¥ÏóêÏÑú ÎÇ¥Ïö© Í∏∞Î∞ò ÌïôÏäµ Í∞ÄÎä•



