# JAEYEON-JO - Momenta Audio Deepfake Detection

## Summary

To briefly summarize, I selected three modelsâ€”ResNet2, LCNN, and Wav2Vec2.0â€”for comparison.  
After reviewing several papers, I found that most models struggle to generalize well.  
Rather than prioritizing maximum accuracy, I focused on **real-time detection** of audio deepfakes.  
If this were a competition or a task purely about performance, I would have chosen heavier models.  
However, if a model can reliably achieve ~90% accuracy, my priority is to **respond quickly** with a lightweight model in real-world scenarios.

---

## Audio Deepfake Detection Models Comparison Table

| Model         | Key Technical Innovation                                       | Reported Performance                       | Why It's Promising                                                                 | Potential Limitations                                                           |
|---------------|---------------------------------------------------------------|---------------------------------------------|--------------------------------------------------------------------------------------|----------------------------------------------------------------------------------|
| **ResNet2**   | - Residual learning<br>- Dilated convolutions & attention pooling | - EER â‰¤ **1.6%** (ADD 2022)<br>- Accuracy **98%+** | - Powerful deep feature extraction<br>- Robust across conditions<br>- Great for conversational detection | - High computational cost<br>- Needs Transformer for long-range context         |
| **LCNN**      | - Lightweight CNN<br>- Max-Feature-Map (MFM) for noise suppression | - EER ~**3â€“5%**<br>- Real-time capable       | - Ideal for **real-time detection**<br>- Edge deployable<br>- Noise-tolerant         | - Limited context modeling<br>- Weaker for complex, long speech segments        |
| **Wav2Vec2.0**| - Self-supervised Transformer<br>- Raw audio input             | - EER â‰¤ **2.2%**<br>- Accuracy **97%+**     | - Captures long context, emotion, and semantics<br>- Strong few-shot performance     | - Large model size<br>- High compute requirement<br>- Hard to use on edge       |

---

### âœ… Why I Chose LCNN

As shown in the table below, LCNN is:
- âœ… Real-time capable  
- âœ… Edge-device friendly  
- âœ… Compatible with log-Mel features  
- âœ… Performs well with lightweight setups

| Model         | Accuracy | EER     | Real-time Capability       | Conversational Analysis     |
|---------------|----------|---------|----------------------------|-----------------------------|
| **ResNet2**   | 98%+     | â‰¤1.6%   | âš ï¸ Moderate (needs optimization) | âœ… Highly suitable           |
| **LCNN**      | ~95%     | 3â€“5%    | âœ… Very high                | âš ï¸ Limited                   |
| **Wav2Vec2.0**| 97%+     | â‰¤2.2%   | âŒ Not suitable             | âœ… Best performance          |

While LCNN may have slightly lower accuracy, itâ€™s the most practical for real-time use.  
Because LCNN lacks deep architecture, I focused on **strong preprocessing** to extract meaningful features.  
I chose `log-Mel + SpecAverage` as my primary preprocessing method, commonly used in research.  
SpecAverage helps preserve feature diversity by masking values with average energy.  
Mel spectrograms are sensitive to duration, so I normalized all inputs to **5 seconds** using padding.

---

## Part 3: Documentation & Analysis

### 1ï¸âƒ£ Implementation Challenges

| **Challenge**                        | **Details**                                                                                                      |
|-------------------------------------|------------------------------------------------------------------------------------------------------------------|
| Large Dataset Handling              | Cached extracted log-Mel spectrograms as `.npy` files to avoid recomputation and speed up training.             |
| File Format Inconsistencies         | Used case-insensitive glob pattern `**/*.[wW][aA][vV]` to handle both `.wav` and `.WAV` files uniformly.        |
| Overfitting Risk & Duration Bias    | Trimmed/padded all audio clips to 5 seconds to remove duration-based bias.                                      |
| Model Generalization                | Used SpecAverage augmentation to help generalize to unseen fake audio.                                          |
| Lack of Real Human Speech Data      | Added LJSpeech dataset to represent genuine human voice and increase dataset realism.                           |

### 2ï¸âƒ£ Solutions to Challenges

| **Challenge**             | **Solution**                                                                 |
|---------------------------|------------------------------------------------------------------------------|
| Large dataset             | Cached `.npy` log-Mel features for reuse                                     |
| File format issues        | Used `**/*.[wW][aA][vV]` glob pattern                                         |
| Audio length bias         | Fixed all audio to 5 seconds via padding/trimming                            |
| Overfitting               | Applied SpecAverage and selected a lightweight LCNN architecture             |
| Lack of real speech data  | Included LJSpeech dataset for authentic audio                                |

### 3ï¸âƒ£ Assumptions Made

- **5 seconds** of audio contains sufficient signal for detecting deepfake cues.
- **log-Mel + SpecAverage** preprocessing captures richer features than MFCC alone.
- **LCNN** is sufficient for baseline detection in real-time applications.
- Case differences in file extensions (e.g., `.wav` vs `.WAV`) are syntactic, not acoustic.

---

### ğŸ“Š Model Pipeline Summary

| Component          | Description                                                                 |
|--------------------|-----------------------------------------------------------------------------|
| **Preprocessing**  | Applied `log-Mel + SpecAverage` using `librosa`.                            |
| **Caching**        | Saved spectrograms as `.npy` for faster future access.                      |
| **Data Split**     | Used stratified `train_test_split` to balance labels.                       |
| **Model**          | LCNN with ReLU activations, 2 CNN layers, and AdaptiveAvgPooling.           |
| **Training/Eval**  | Used `BCELoss` and `Adam`; tracked AUC, ACC, and EER per epoch.             |
| **Visualization**  | Saved performance bar chart as `lcnn_performance.png`.                      |
| **Output**         | Printed metrics to console; exported visualization image.                   |

---

### âœ… High-Level Model Description

- **Input:** Each 5-second audio clip is transformed into a log-Mel spectrogram with SpecAverage masking.
- **Architecture:** LCNN with 2 convolutional layers + ReLU + BatchNorm + AdaptiveAvgPool, followed by a fully connected layer with Sigmoid.
- **Loss:** Binary Cross-Entropy (BCELoss)
- **Metrics:**  
  - AUC: Area under the ROC curve  
  - ACC: Classification accuracy  
  - EER: Equal Error Rate â€” when False Positive Rate equals False Negative Rate

---

### âœ… Performance Results

- **LCNN Epoch 20:**  
  `Loss = 0.6353`  
  `AUC = 0.7233`  
  `ACC = 0.5421`  
  `EER = 0.3371`

---

### ğŸ” Strengths & Weaknesses

#### âœ… Strengths

- **Lightweight & Fast:** Suitable for edge deployment with limited resources.  
- **Effective on Short Clips:** Handles 5-second inputs well.  
- **Interpretable:** Simple architecture makes debugging and visualization easier.

#### âš ï¸ Weaknesses

- **Overfitting Risk:** Tends to memorize small datasets.  
- **Limited Context:** LCNN lacks long-range modeling like Transformers.  
- **Performance Ceiling:** Struggles to surpass 75â€“80% AUC without richer features or ensembles.

---

### âœ… Suggestions for Future Improvements

- Use more realistic datasets: **ASVspoof**, **FakeAVCeleb**, call center audio.
- Add data from **noisy, multilingual, and modern TTS** environments (e.g., VITS, Bark, Vall-E).
- Incorporate advanced augmentation: SpecMix, pitch shifting, time-stretching, background noise.

---

## 3ï¸âƒ£ Reflection

### Q1 & Q2: Major Challenges & Real-World Performance

| Category                | Challenge / Consideration                                                       | Solution / Implementation                                                  |
|-------------------------|----------------------------------------------------------------------------------|-----------------------------------------------------------------------------|
| File Format Issues   | Case mismatch in `.wav` vs `.WAV` created loading bugs                          | Case-insensitive loader and file handling                                   |
| Preprocessing Bias   | Model overfitting to audio duration rather than content                         | Applied fixed-length 5s input + SpecAverage augmentation                    |
| Model Selection       | Needed a balance of speed vs performance for production                         | Chose LCNN for fast, efficient deployment                                   |
| Realism in Dataset   | Many datasets are synthetic only                                                | Included real human speech via LJSpeech dataset                             |

---

### Q3: What Additional Data Would Improve Performance?

- **Noisy real-world audio** (e.g., phone calls, YouTube, podcasts)  
- **Modern fake audio** (e.g., Vall-E, Bark, VITS samples)  
- **Multilingual datasets** to improve generalization  
- **Augmentations** like SpecMix, pitch shifting, time-stretching, noise injection

---

### Q4: Deployment Strategy

#### Dataset Expansion
- Add data from diverse sources, languages, and TTS models for generalization.

####  Cloud-Based Automation
- Use **AWS SageMaker** or **Azure ML Pipelines** to automate training and deployment workflows.
- Store and manage datasets in **S3** or **Azure Blob Storage**.

#### Inference Pipeline
- Apply 5-second **sliding window** over long audio for real-time chunk-wise prediction.
- Aggregate outputs for final decisions.

#### Edge Deployment
- Deploy **LCNN** on mobile or embedded devices for fast inference:
  - Mobile phones (iOS/Android)
  - Raspberry Pi / Jetson / IoT devices
- Convert model to **ONNX**, **TorchScript**, or **TFLite**.

#### Monitoring & Retraining
- Set up real-time dashboards (CloudWatch, Grafana)  
- Monitor prediction confidence and drift  
- Retrain on new samples as synthesis methods evolve




# JAEYEON-JO-Momenta

ê°„ë‹¨í•˜ê²Œ ìš°ì„  ì„¤ëª…ì„ í•˜ìë§Œ, ë‚˜ëŠ” ResNet2, LCNN, Wav2Vec2.0ë¥¼ ì„ ì •í•˜ì˜€ë‹¤. 
ë…¼ë¬¸ì„ ì½ì–´ë³´ë‹ˆ, ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì´ ì•„ì§ê¹Œì§€ëŠ” ì¼ë°˜í™” ì‹œ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ê²ƒì„ ë³´ê³  ì„±ëŠ¥ì˜ ìš°ì„ ë³´ë‹¤ëŠ”  real-timeì—ì„œ deepfakeë¥¼ ì‹¤ì‹œê°„ ê°ì§€í•˜ëŠ” ê²ƒì— ìµœìš°ì„ ì„ ë‘ì—ˆë‹¤. ë§Œì•½ ì´ê²ƒì´ ëŒ€íšŒê±°ë‚˜ ì„±ëŠ¥ í–¥ìƒì—ë§Œ ì´ˆì ì„ ë§ì·„ìœ¼ë©´ computationì´ ë§ì´ ìš”êµ¬ë˜ëŠ” ëª¨ë¸ì„ ì‚¬ìš©í–ˆì„ ê²ƒì¸ë°, ê·¸ê²Œ ì•„ë‹ˆë¼, ì •í™•ë„ê°€ ì•½ 90%ê¹Œì§€ í™•ë³´ë¬ë‹¤ê³  ìƒê°í•˜ë©´ ê·¸ëƒ¥ ì‹¤ì‹œê°„ ì •ë³´ì— ë¹ ë¥´ê²Œ ëŒ€ì‘í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ì„ ì •í–ˆë‹¤. 

## ğŸ” Audio Deepfake Detection Models Comparison Table ì•„ë˜ëŠ” ë…¼ë¬¸ì„ ì½ê³  ë‚´ê°€ 3ê°œ ì„ íƒí•œ ëª¨ë¸ ë¶„ì„ì´ë‹¤. 

| Model       | Key Technical Innovation | Reported Performance | Why It's Promising | Potential Limitations |
|-------------|---------------------------|-----------------------|----------------------|------------------------|
| **ResNet2** | - Residual learning structure<br>- Can incorporate dilated convolution & attention pooling | - EER â‰¤ **1.6%** (ADD 2022)<br>- Accuracy **98%+** | - Strong deep feature extraction<br>- Robust across varied conditions<br>- Well-suited for conversation-based detection | - High computational cost<br>- Limited ability to capture long-term context (needs Transformer integration) |
| **LCNN**    | - Lightweight CNN with Max-Feature-Map (MFM)<br>- Effective noise suppression | - EER around **3â€“5%**<br>- Real-time capable | - Excellent for **real-time detection**<br>- Suitable for edge devices<br>- Performs well in noisy environments | - Lacks deep contextual understanding when used alone<br>- Not ideal for complex conversations |
| **Wav2Vec2.0** | - Self-supervised learning<br>- Raw audio input + Transformer-based architecture | - EER â‰¤ **2.2%**<br>- Accuracy **97%+** | - Outstanding at capturing long context, emotion, and meaning<br>- Performs well with **limited labeled data**<br>- Ideal for real conversation analysis | - Large model size and high compute cost<br>- Requires optimization for real-time deployment |

---
### ğŸ” ê·¸ë¦¬ê³  ë‚˜ëŠ” ì´ ì¤‘ì—ì„œ LCNNì„ ì„ íƒí–ˆë‹¤. Why my choice is LCNN?
ì•„ë˜ í‘œì—ì„œ ë³´ì´ë“¯, 
- âœ… Real-time capable  
- âœ… Easy to deploy on edge devices  
- âœ… Compatible with log-Mel features  
- âœ… Performs well with low-compute environments

| Model       | Accuracy | EER     | Real-time Capability | Conversational Analysis |
|-------------|----------|---------|------------------------|--------------------------|
| ResNet2     | 98%+     | â‰¤1.6%   | âš ï¸ Moderate (needs optimization) | âœ… Highly suitable |
| LCNN        | ~95%     | 3â€“5%    | âœ… Very high           | âš ï¸ Limited |
| Wav2Vec2.0  | 97%+     | â‰¤2.2%   | âŒ Needs high resources | âœ… Best performance |



ë³´ë©´ LCNNì€ ì •í™•ë„ê°€ 95%ê¹Œì§€ ì˜¬ë¼ê°€ê³  Very high real-time capacityê°€ ìˆë‹¤ê³  ìƒê°ë˜ì–´ LCNN ëª¨ë¸ì„ ì„ íƒí–ˆë‹¤. 
ëŒ€ì‹ , ì „ì²˜ë¦¬ê°€ ì¤‘ìš”í•˜ì—¬ ì „ì²˜ë¦¬ë¥¼ ì˜í•˜ë ¤ê³  ë…¸ë ¥í–ˆë‹¤.
ëŒ€ëµì ìœ¼ë¡œ í¬ê²Œ ì „ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì€ MFCC, Mel, SpecAverageê°€ ë…¼ë¬¸ë³´ë‹ˆê¹Œ ë§ì´ ì“°ì˜€ë”ë¼. ê·¸ë˜ì„œ ë‚˜ë„ ì´ ë°©ë²•ì„ í†µí•´ ì „ì²˜ë¦¬ í•˜ë ¤ê³  í–ˆë‹¤. 
LCNNì€ ì •í™•ë„ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ ëª¨ë¸ì´ë¼ ì „ì²˜ë¦¬ë¥¼ í†µí•´ ìµœëŒ€í•œ ë§ì€ feature íŠ¹ì§•ë“¤ì„ ë½‘ìœ¼ë ¤ê³  í–ˆë‹¤. 
ê·¸ë˜ì„œ 1ë²ˆì˜ ì „ì²˜ë¦¬ë³¸ë‹¤ëŠ” log-Mel + SpecAvaerge ì „ì²˜ë¦¬ë¥¼ ì„ íƒí–ˆë‹¤. SpecAverageë¥¼ ì„ íƒí•œ ê²ƒì€ ë§ˆìŠ¤í‚¹ì„ í‰ê· ìœ¼ë¡œ ì¡°ì ˆí•˜ë©´ì„œ íŠ¹ì„±ì„ ì˜ ë½‘ì„ ìˆ˜ ìˆê¸°ì— ì„ íƒí–ˆë‹¤. 
ëŒ€ì‹  Melì€ ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë¯¼ê°í•˜ê¸°ì— ëª¨ë“  ì˜¤ë””ì˜¤ë¥¼ 5ì´ˆë¡œ í†µì¼í•˜ë ¤ê³  í–ˆê³  paddingí•¨ 


## Part 3: Documentation & Analysis

1. Document your implementation process, including:
    - Any challenges encountered

| **Challenge**                       | **Details**                                                                                                       |
|------------------------------------|-------------------------------------------------------------------------------------------------------------------|
| **Large Dataset Handling**         | Cached extracted log-Mel spectrograms as `.npy` files to avoid recomputation and reduce training time.           |
| **File Format Inconsistencies**    | Used case-insensitive glob pattern `**/*.[wW][aA][vV]` to handle `.wav` and `.WAV` files uniformly.               |
| **Overfitting Risk & Duration Bias** | Applied fixed-length preprocessing: all audio clips were trimmed or zero-padded to 5 seconds to remove bias.     |
| **Model Generalization**           | Used SpecAverage augmentation to reduce overfitting and promote robustness in learning.                          |
| **Lack of Real Human Speech Data** | Added the LJSpeech dataset to introduce authentic human voice samples and improve realism.                       |
      
    - How you addressed these challenges
 | **Challenge**            | **Solution**                                                                 |
|--------------------------|------------------------------------------------------------------------------|
| Large dataset            | Used **caching (`.npy`)** of extracted log-Mel features to avoid recomputation |
| File format issues       | Implemented case-insensitive glob pattern: `**/*.[wW][aA][vV]`                |
| Audio length bias        | All audio was **padded or trimmed to 5 seconds** using fixed-length preprocessing |
| Overfitting              | Applied **SpecAverage augmentation** and used a **lightweight CNN model (LCNN)** |
| Real speech data scarcity | Added **LJSpeech dataset** to represent real human voice                    |
   

    - Assumptions made
    - **5 seconds of audio** is enough to capture the key acoustic features necessary to distinguish between real and fake speech.
- Using **log-Mel spectrograms combined with SpecAverage masking** improves generalization and reduces overfitting compared to MFCCs alone.
- A **lightweight architecture (LCNN)** is sufficient for baseline detection without requiring large-scale Transformer-based models like Wav2Vec 2.0.
- Both `.wav` and `.WAV` formats are **acoustically identical**; the issue is purely syntactic and handled during file loading.

2. Include an analysis section that addresses:
ğŸ“Š Model Analysis
| êµ¬ì„± ìš”ì†Œ     | ì„¤ëª… |
|----------------|------|
| **ì „ì²˜ë¦¬**      | `log-Mel + SpecAverage` ì ìš©. `librosa`ë¥¼ ì´ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ë¥¼ ë¡œë”©í•˜ê³  ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ë³€í™˜ ìˆ˜í–‰. |
| **ìºì‹±**        | `.npy` í˜•íƒœë¡œ ì €ì¥í•˜ì—¬ ì¬ì‚¬ìš© ê°€ëŠ¥, ë¹ ë¥¸ ë°ì´í„° ë¡œë”© ê°€ëŠ¥. |
| **ë°ì´í„° ë¶„ë¦¬** | `train_test_split`ì— ê¸°ë°˜í•˜ì—¬ stratified split ì ìš©, í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€. |
| **ëª¨ë¸ êµ¬ì¡°**   | LCNN ê¸°ë°˜ êµ¬ì¡°. `ReLU` í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš©. 2ì¸µ CNN + `AdaptiveAvgPool`ë¡œ ì¶œë ¥ ì •ê·œí™”. |
| **í›ˆë ¨/í‰ê°€ ë£¨í”„** | `BCELoss`ì™€ `Adam` Optimizer ì‚¬ìš©. ë§¤ epochë§ˆë‹¤ `AUC`, `ACC`, `EER` ì§€í‘œ ì¶œë ¥. |
| **ì‹œê°í™”**     | ìµœì¢… ì„±ëŠ¥ì„ `ë§‰ëŒ€ê·¸ë˜í”„`ë¡œ ì‹œê°í™”í•˜ì—¬ `lcnn_performance.png`ë¡œ ì €ì¥. |
| **ìµœì¢… ì¶œë ¥**   | ì½˜ì†”ì— í‰ê°€ ì§€í‘œ ì¶œë ¥ + ì´ë¯¸ì§€ íŒŒì¼ ì €ì¥ (`lcnn_performance.png`). |

### âœ… Why LCNN Was Selected

- **Real-time capability:**  
  LCNN (Lightweight Convolutional Neural Network) is fast and computationally efficientâ€”ideal for edge or mobile deployment.

- **Proven effectiveness:**  
  LCNN has been shown to perform well in fake audio detection tasks with relatively low error rates.

- **Compatibility:**  
  Works well with log-Mel spectrograms and benefits from data augmentations like SpecAverage.

- **Interpretability:**  
  Easier to visualize and understand than Transformer-based models.

### âœ… How the model works (high-level technical explanation)

- **Input:** Each audio file is converted into a log-Mel spectrogram, augmented using SpecAverage masking, and reshaped to a fixed length (5 seconds)
- **Model Architecture:** Two convolutional layers with ReLU, followed by BatchNorm and Adaptive Average Pooling.Fully connected layer with a sigmoid activation outputs the probability of the audio being fake.
- **Loss Function:** Binary Cross-Entropy Loss (BCELoss) is used since this is a binary classification problem (real vs. fake).
- **Evaluation Metrics:** AUC (Area Under ROC Curve): How well the model separates the two classes, Accuracy: Proportion of correct predictions, EER (Equal Error Rate): The point where False Positive Rate = False Negative Rate.

### âœ… Performance results on your chosen dataset
- #### LCNN Epoch 20: Loss=0.6353, AUC=0.7233, ACC=0.5421, EER=0.3371
  
### ğŸ” Strengths & Weaknesses
#### ğŸŸ¢ Strengths

- **Lightweight & Fast**  
  Can be deployed in low-resource environments.

- **Effective on Short Clips**  
  Works reasonably well with 5-second samples.

- **Interpretable**  
  Simpler architecture makes debugging and tuning easier.

#### ğŸ”´ Weaknesses

- **Overfitting**  
  Model quickly memorizes small datasets.

- **Limited Context**  
  LCNN lacks long-range temporal modeling like Transformers.

- **Performance Ceiling**  
  Struggles to exceed 75â€“80% AUC without ensemble or richer features.

### âœ… Suggestions for future improvements
- Improve Dataset Quality & Variety:
  Use ASVspoof, FakeAVCeleb, and call center-style datasets for better generalization.
  Add noisy environments, different languages, and newer fake voice synthesis methods (VITS, Bark, Vall-E).


## 3. Reflection questions to address:

### Q1: What were the most significant challenges? & Q2: How might this approach perform in the real world?

| Category                | Challenge / Consideration                                                     | Solution / Implementation                                                                 |
|-------------------------|-------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|
| ğŸ”„ File Format Issues   | Large dataset size, case-sensitive `.wav` vs `.WAV` caused read errors       | Used consistent loader logic and added compatibility for both `.wav` and `.WAV` extensions  |
| âš™ï¸ Preprocessing Dilemma | Model overfitting, generalization problems, bias due to length differences   | Applied `log-Mel + SpecAverage`, and fixed all audio to **5 seconds** using pad/crop       |
| âš–ï¸ Model Selection       | Trade-off between inference speed and accuracy                               | Chose **LCNN** for fast, real-time capable, lightweight architecture                        |
| ğŸ” Real Data Requirement | Needed realistic audio (not just synthetic/generated)                        | Added **LJSpeech** dataset as genuine human voice source                                    |
---

### Q3: What additional data would improve performance?

- **Noisy and real-life audio**  
  e.g., phone calls, YouTube recordings, podcast snippets.

- **Fake audio from modern generators**  
  Incorporate samples from models like **Vall-E**, **Bark**, or **VITS** to stay up to date with the latest synthesis techniques.

- **Multilingual data**  
  To ensure the model generalizes across different languages and accents.

- **Advanced data augmentation techniques**  
  Such as: SpecMix, Pitch shift, Time-stretching, Background noise injection
---

### Q4: How would you deploy this?

#### ğŸ“¦ Dataset Expansion for Generalization
- **Add more diverse data** to improve model generalization:
  - Noisy real-world audio (e.g., phone calls, podcasts, YouTube)
  - Multiple languages and accents
  - Fake audio from modern TTS models (e.g., Vall-E, Bark, VITS)
- Ensure dataset includes varied recording environments, devices, and speakers.

#### ğŸ¤– Model Training Automation (Cloud)
- Use **cloud platforms like AWS or Azure** to automate model training, evaluation, and deployment:
  - Leverage **AWS SageMaker** or **Azure ML Pipelines**

#### ğŸ§  Inference Pipeline
- Implement **batch inference** for stored datasets and **real-time streaming inference** for live applications.

#### ğŸ“± Edge Deployment
- Use the **LCNN model** for **low-latency, lightweight inference** on edge devices:
  - Mobile phones (iOS/Android)
  - Embedded systems (Raspberry Pi, NVIDIA Jetson)
  - IoT devices (smart assistants, surveillance)

- Convert model to **ONNX**, **TorchScript**, or **TFLite** for optimal performance on various platforms.

#### ğŸ“Š Monitoring & Retraining
- **Continuously monitor**:
  - Prediction confidence levels
  - Accuracy drift over time
  - New types of fake audio
- Set up **automated retraining pipelines** triggered by performance degradation or new data.
- Use dashboards to visualize performance (e.g., via Amazon CloudWatch, Azure Monitor, or Grafana).
---
























## ğŸ“š Appendix

### ğŸ“ Dataset Info

- **Real**: LJSpeech (~13,000 clips, ~10 seconds each)  
- **Fake**: MelGAN-generated audio (~5 seconds)  
- âœ… Class-balanced cache system implemented

---

### ğŸ§ª Training Pipeline Highlights

- `extract_logmel()` ensures uniform audio duration  
- Padding applied to ensure consistent input dimensions  
- `pad_collate_fn()` used for batch-wise padding  
- Still requires **early stopping or regularization**

---

## âœ… Summary Table

| Feature                | LCNN          | ResNet2        | Wav2Vec2.0     |
|------------------------|---------------|----------------|----------------|
| Inference Speed        | âš¡ Very fast   | âš ï¸ Moderate     | ğŸ¢ Very slow    |
| Accuracy (Expected)    | ğŸ‘ ~95%        | âœ… 98%+         | âœ… 97%+         |
| EER (Expected)         | âš ï¸ 3â€“5%        | âœ… â‰¤1.6%        | âœ… â‰¤2.2%        |
| Suitable for Streaming | âœ… Yes         | âš ï¸ With tuning  | âŒ Not ideal    |

---

## ğŸš€ Future Work

- âœ… Integrate **multi-model ensemble** (LCNN + ResNet2 + distilled Wav2Vec2.0)  
- âœ… Fine-tune on **ASVspoof 2021**, **FakeAVCeleb**, or more recent datasets  
- âœ… Experiment with **attention pooling**, **SpecMix**, **SpecAugment++**  
- âœ… Build a **frontend dashboard** for live detection and visualization

---





# ğŸ§ Audio Deepfake Detection using log-Mel + SpecAverage + LCNN

> Lightweight audio deepfake detection with strong real-time capability and generalization focus.  
> Explored multiple models and preprocessing methods.  
> Final implementation uses LCNN with SpecAverage-augmented log-Mel features.

---

## ğŸ“Œ Overview

This project focuses on building an efficient and practical pipeline for detecting AI-generated human speech using lightweight CNN-based models. The goal is real-time applicability while maintaining robust detection capability.

---

## 1. âš™ï¸ Implementation Documentation

### âœ… Challenges Encountered

1. **Large Fake Dataset**
   - Slow preprocessing and loading due to dataset size
   - Limited generalization: all audio was generated
   - Real vs. fake audio had different durations (10s vs. 5s)

2. **Wav2Vec2.0 Inference Cost**
   - Too large for real-time or mobile deployment
   - Slower than lightweight CNNs like LCNN and ResNet2

3. **File Extension Issues**
   - `.wav` vs `.WAV` (case-sensitive) caused loading failures

4. **Overfitting Risk**
   - Small dataset + deep CNNs = higher chance of memorization

5. **Preprocessing Dilemma**
   - MFCC compresses too much
   - Log-Mel is better but vulnerable to overfitting
   - SpecAverage helps with augmentation but needs tuning

6. **Length-Based Bias**
   - Model might learn to classify based on input length (real=10s, fake=5s)

---

### ğŸ› ï¸ How These Were Addressed

- âœ… Added real audio: **LJSpeech** dataset
- âœ… Switched to lightweight models: **LCNN**, **ResNet2**
- âœ… Preprocessing: used **log-Mel + SpecAverage**
- âœ… Fixed-length audio: trimmed/padded all to **5 seconds**
- âœ… Created custom `extract_logmel()` with duration constraint

```python
def extract_logmel(path, sr=16000, n_mels=40, duration=5.0):
    y, _ = librosa.load(path, sr=sr)
    target_len = int(sr * duration)
    y = np.pad(y, (0, max(0, target_len - len(y))))[:target_len]
    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)
    logmel = librosa.power_to_db(mel)
    return torch.tensor(spec_average_augment(logmel)).float().unsqueeze(0)


# ğŸ§¾ Part 3: Documentation & Analysis

## 1. Implementation Documentation

### âœ… Challenges Encountered

1. **ë°ì´í„°ì…‹ì˜ í¬ê¸° ë¬¸ì œ**  
   ë‹¤ìš´ë¡œë“œí•œ fake ì˜¤ë””ì˜¤ ë°ì´í„°ì…‹ì´ ë„ˆë¬´ ì»¤ì„œ ì „ì²˜ë¦¬ì™€ ë¡œë”©ì— ì˜¤ëœ ì‹œê°„ì´ ê±¸ë ¸ê³ , ëª¨ë‘ generated audioë¼ì„œ ì¼ë°˜í™”ì— ì œí•œì´ ìˆì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
   ì‹¤ì œë¡œ ëª¨ë¸ì„ ëŒë ¤ë³¼ ì‹œê°„ì´ ë§ì´ ì—†ì—ˆìŒ. 

2. **Wav2Vec 2.0 ëª¨ë¸ì˜ ì‹¤ì‹œê°„ ì²˜ë¦¬ í•œê³„**  
   Wav2Vec2.0ì€ powerfulí•˜ê¸´ í•˜ì§€ë§Œ, í¬ê¸°ê°€ ì»¤ì„œ real-time ì ìš©ì—ëŠ” ë¶€ì í•©í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ëª¨ë°”ì¼/ê²½ëŸ‰í™”ê°€ ì¤‘ìš”í•œ í™˜ê²½ì—ì„œëŠ” ì‚¬ìš©ì´ ì–´ë ¤ì›€.

3. ë°ì´í„° ì „ì²˜ë¦¬: ê³¼ì œë¥¼ ìœ„í•œ ë…¼ë¬¸ë“¤ì—ì„œ ëŒ€ë¶€ë¶„ì˜ audio deepfake detection modelì˜ í•œê³„ì ì„ ì¼ë°˜í™”ë¼ê³  í•¨. ì¦‰,
   ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì–´ë–»ê²Œ í•´ì•¼ ì¼ë°˜í™” ì‹œ ë–¨ì–´ì§€ëŠ” ì„±ëŠ¥ì„ ìµœëŒ€í•œ ë°©ì§€í•  ìˆ˜ ìˆì„ê¹Œ ê³ ë¯¼í•¨. Noiseê°€ ì—†ëŠ” ê³ ì„±ëŠ¥ íŒŒì¼ë§ê³  noiseê°€ ìˆëŠ” íŒŒì¼, ê·¸ë¦¬ê³  ë˜ ìµœì‹ ì˜ ê¸°ìˆ ë¡œ ìƒì„±ëœ deepfake audio ë“± ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ í™•ë³´ê°€ í•„ìš”í•¨ 
   MFCC:
   Mel:
   SpecAverage:  ìœ„ì˜ 3ê°€ì§€ ì „ì²˜ë¦¬ ë°©ë²•ìœ¼ë¡œëŠ” ëª¨ë“  deepfake audio detectionì— í•œê³„ê°€ ìˆìŒ. 
 
4. ì¼ë°˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì‚¬ìš©í•˜ì§€ ì•Šê¸°. 
   ì¼ë°˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë‹ˆê¹Œ í™œì„±í™” í•¨ìˆ˜ê°€ gradientë¥¼ 0ìœ¼ë¡œ ë§Œë“¬. ì¦‰, ê¹Šì´ ìˆëŠ” ì‹ ê²½ë§ì„ ê±´ì¶•í•  ìˆ˜ê°€ ì—†ìŒ. 
---

### âœ… How Challenges Were Addressed

1. **ì‹¤ì œ(real) ì˜¤ë””ì˜¤ ë°ì´í„°ì…‹ ì¶”ê°€ í™•ë³´**  
   LJSpeech ë°ì´í„°ì…‹(`"/content/drive/MyDrive/Internship /LJSpeech-1.1/wavs"`)ì„ ì‚¬ìš©í•´ ì¼ë°˜ ìŒì„±ê³¼ ë¹„êµ ê°€ëŠ¥í•˜ë„ë¡ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.

2. **Wav2Vec 2.0 ì œê±°**  
   ì´ˆê¸°ì—ëŠ” Wav2Vec2.0ì„ transfer learning í˜•íƒœë¡œ feature extractorë¡œ ì‚¬ìš©í•˜ë ¤ í–ˆì§€ë§Œ, ëª¨ë¸ì´ ë„ˆë¬´ ì»¤ì„œ ì†ë„ì™€ ì‹¤ìš©ì„± ë©´ì—ì„œ ë¶€ì í•©í•˜ë‹¤ê³  íŒë‹¨.  
   **ì •í™•ë„ ì°¨ì´ê°€ í¬ì§€ ì•Šë‹¤ë©´, ì‹¤ìš©ì ì´ê³  ë¹ ë¥¸ ëª¨ë¸ì´ ìƒìš©í™”ì— ë” ì í•©**í•˜ë‹¤ê³  ìƒê°í•˜ì—¬ LCNNê³¼ ResNetV2ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë¹„êµ ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

3. **ë°ì´í„° ì „ì²˜ë¦¬**
   ë§ì€ ë°ì´í„° ì „ì²˜ë¦¬ ê¸°ë²•ì´ ë…¼ë¬¸ì— ìˆì—ˆë‹¤. MFCC, Log-Mel, SpecAverage. ì—¬ê¸°ì„œ ê°€ì¥ ì •í™•í•œ ì „ì²˜ë¦¬ ë°©ë²•ì„ ì°¾ì•„ì•¼ í–ˆì—ˆë‹¤.
   ì™œëƒë©´ ì´ë¯¸ Wav2Vec2.0ë¥¼ ì œê±° í–ˆê¸°ì— ê°€ëŠ¥í•œ ë§ì€ feature extractionì„ í–ˆì–´ì•¼ í–ˆë‹¤.  
   ê·¸ë¦¬ê³  
   - MFCC
   - Log-Mel
   - SpecAverage
  
4. ë”¥ëŸ¬ë‹ ì‚¬ìš©(light version)
   ê¹Šì´ ìˆëŠ” ì‹ ê²½ë§ì´ ìˆì„ ìˆ˜ë¡ í•™ìŠµì„ ë” ì˜í•  ìˆ˜ ìˆê¸°ì— ì´ë¥¼ ì˜ í•  ìˆ˜ ìˆê¸°ì— LCNN ì‚¬ìš©í•¨.
   
---

## Part 3: Documentation & Analysis

1. Document your implementation process, including:
    - Any challenges encountered:
   1. ë‹¤ìš´ë¡œë“œí•œ fake ì˜¤ë””ì˜¤ ë°ì´í„°ì…‹ì´ ë„ˆë¬´ ì»¤ì„œ ì „ì²˜ë¦¬ì™€ ë¡œë”©ì— ì˜¤ëœ ì‹œê°„ì´ ê±¸ë ¸ê³ , ëª¨ë‘ generated audioë¼ì„œ ì¼ë°˜í™”ì— ì œí•œì´ ìˆì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
   2. Wav2Vec2.0ì€ powerfulí•˜ê¸´ í•˜ì§€ë§Œ, í¬ê¸°ê°€ ì»¤ì„œ real-time ì ìš©ì—ëŠ” ë¶€ì í•©í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ëª¨ë°”ì¼/ê²½ëŸ‰í™”ê°€ ì¤‘ìš”í•œ í™˜ê²½ì—ì„œëŠ” ì‚¬ìš©ì´ ì–´ë ¤ì›€.
  
    - How you addressed these challenges:
    - 1. ì‹¤ì œ ë°ì´í„° ì…‹ì„ êµ¬í–ˆë‹¤ - ("/content/drive/MyDrive/Internship /LJSpeech-1.1/wavs")ì½”ë“œì— ìˆìŒ),
    - 2. ì´ˆê¸°ì—ëŠ” Wav2Vec2.0ì„ transfer learning í˜•íƒœë¡œ feature extractorë¡œ ì‚¬ìš©í•˜ë ¤ í–ˆì§€ë§Œ, ëª¨ë¸ì´ ë„ˆë¬´ ì»¤ì„œ ì†ë„ì™€ ì‹¤ìš©ì„± ë©´ì—ì„œ ë¶€ì í•©í•˜ë‹¤ê³  íŒë‹¨.
ì •í™•ë„ ì°¨ì´ê°€ í¬ì§€ ì•Šë‹¤ë©´, ì‹¤ìš©ì ì´ê³  ë¹ ë¥¸ ëª¨ë¸ì´ ìƒìš©í™”ì— ë” ì í•©í•˜ë‹¤ê³  ìƒê°í•˜ì—¬ LCNNê³¼ ResNetV2ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë¹„êµ ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤
   - LCNN ì…ë ¥í•  ë•Œ, featureì˜ í¬ê¸°ê°€ ë‹¤ë¦„.
   - ìƒˆë¡œ ì…ë ¥í•œ ë°ì´í„°ì…‹ ì˜¤ë””ì˜¤ ê¸¸ì´ê°€ ê¸¸ì—ˆìŒ.  
    - Assumptions made:  
  
3. Include an analysis section that addresses:
    - Why you selected this particular model for implementation: ê²°ê³¼ë¥¼ ë³´ë‹ˆ ì•™ìƒë¸”ì´ ì¢‹ë‹¤ëŠ” íš¨ê³¼ë¥¼ ë´¤ë‹¤
    - How the model works (high-level technical explanation): SpecAverage ì „ì²˜ë¦¬í•˜ê³  logMELì„ í•¨. ì›ë˜ëŠ” MFCCë¥¼ í•˜ë ¤ê³  í–ˆëŠ”ë°, ê·¼ë° ë„ˆ ê°™ìœ¼ë©´ ì™œ SpecAverge+logMelì„ ì„ íƒí•´? over MFCC? 
    - Performance results on your chosen dataset: ì•„ì§ ëª¨ë¸ ëŒë¦¬ëŠ” ì¤‘ 
    - Observed strengths and weaknesses: 
    - Suggestions for future improvements: 

4. Reflection questions to address:
    1. What were the most significant challenges in implementing this model? 
    2. How might this approach perform in real-world conditions vs. research datasets?
    3. What additional data or resources would improve performance?
    4. How would you approach deploying this model in a production environment?
  
   í™•ì¥ì ã……ã…‚....wav. WAV ì°¨ì´ê°€ ìˆì„ì§€...

   ğŸ¯ ë¬¸ì œ ë‹¤ì‹œ ì •ë¦¬:
real ì˜¤ë””ì˜¤ëŠ” ì•½ 10ì´ˆ, fake ì˜¤ë””ì˜¤ëŠ” ì•½ 5ì´ˆ
â†’ ëª¨ë¸ì´ "ê¸¸ì´ ì°¨ì´"ë§Œìœ¼ë¡œ ì§„ì§œ/ê°€ì§œë¥¼ ë¶„ë¥˜í•  ìœ„í—˜
â†’ ì¼ë°˜í™” ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥

def extract_logmel(path, sr=16000, n_mels=N_MELS):
    y, _ = librosa.load(path, sr=sr)
    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)
    ...
    ë”°ë¼ì„œ log-Melë„ ì‹œê°„ ì¶•(t) ê¸¸ì´ê°€ ì„œë¡œ ë‹¬ë¼ì§‘ë‹ˆë‹¤.
    íŒ¨ë”©: ì§§ì€ ì˜¤ë””ì˜¤ íŒŒì¼ ë’¤ì— 0 ê°’ì„ ì±„ì›Œ ê¸¸ì´ë¥¼ ë§ì¶”ëŠ” ë°©ë²•
    ë¬¸ì œ ìˆìŒ: librosa.load()ëŠ” ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë”°ë¼ ê·¸ëŒ€ë¡œ ë¡œë”©í•©ë‹ˆë‹¤.

ì¦‰, realì€ 10ì´ˆ â†’ 160,000 samples, fakeëŠ” 5ì´ˆ â†’ 80,000 samples

ë”°ë¼ì„œ log-Melë„ ì‹œê°„ ì¶•(t) ê¸¸ì´ê°€ ì„œë¡œ ë‹¬ë¼ì§‘ë‹ˆë‹¤.

def pad_collate_fn(batch):
    features, labels = zip(*batch)
    max_len = max([f.shape[-1] for f in features])
    padded = [torch.nn.functional.pad(f, (0, max_len - f.shape[-1])) for f in features]
    return torch.stack(padded), torch.tensor(labels).float()
ë¶€ë¶„ì ìœ¼ë¡œ í•´ê²°ë¨: ë°°ì¹˜ ë‹¨ìœ„ì—ì„œëŠ” spectrogramì˜ ê¸¸ì´ë¥¼ íŒ¨ë”©ìœ¼ë¡œ ë§ì¶”ê³  ìˆìŒ

í•˜ì§€ë§Œ ì›ë˜ ê¸¸ì´ì— ë”°ë¥¸ ì •ë³´ëŠ” ì—¬ì „íˆ featureì— ë‚¨ì•„ ìˆìŒ

ì¦‰, ëª¨ë¸ì´ ì—¬ì „íˆ **"ì§§ì€ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì´ë©´ fake, ê¸¸ë©´ real"**ì´ë¼ëŠ” íŒíŠ¸ë¥¼ í•™ìŠµí•  ê°€ëŠ¥ì„± ìˆìŒ
â†’ í•™ìŠµì´ ê¸¸ì´ì— biasë  ìˆ˜ ìˆê³ , ì´ëŠ” generalizationì— ì·¨ì•½ì ì´ ë©ë‹ˆë‹¤.


ëª¨ë“  ì˜¤ë””ì˜¤ë¥¼ 5ì´ˆë¡œ ì˜ë¼ì„œ ê³ ì • â†’ ê¸¸ì´ ì •ë³´ê°€ ëª¨ë¸ì— ë°˜ì˜ë˜ì§€ ì•ŠìŒ

ì§„ì§œ/ê°€ì§œ ëª¨ë‘ ê°™ì€ ê¸¸ì´ë¡œ ë¹„êµë˜ë¯€ë¡œ ê¸¸ì´ì— ì˜ì¡´í•œ ë¶„ë¥˜ ê°€ëŠ¥ì„± ì°¨ë‹¨
âœ… ê²°ë¡ 
í•­ëª©	í˜„ì¬ ì½”ë“œ	í•´ê²° ì—¬ë¶€
ì˜¤ë””ì˜¤ ê¸¸ì´ í†µì¼	âŒ (librosa.load()ë¡œ ì›ë³¸ ê¸¸ì´ ìœ ì§€)	í•´ê²° ì•ˆ ë¨
ê¸¸ì´ ë§ì¶¤ ì²˜ë¦¬	â­• (pad_collate_fnìœ¼ë¡œ íŒ¨ë”©)	ì¼ë¶€ í•´ê²° (ëª¨ì–‘ ë§ì¶¤ë§Œ)
ê¸¸ì´ bias ì œê±°	âŒ ëª¨ë¸ì´ ì—¬ì „íˆ ê¸¸ì´ ì •ë³´ì— ì ‘ê·¼ ê°€ëŠ¥	ìˆ˜ì • í•„ìš”


âœ… ìˆ˜ì •ëœ extract_logmel() í•¨ìˆ˜
python
ë³µì‚¬
í¸ì§‘
def extract_logmel(path, sr=16000, n_mels=N_MELS, duration=5.0):
    y, _ = librosa.load(path, sr=sr)

    # ê¸¸ì´ë¥¼ 5ì´ˆë¡œ ê³ ì • (ë¶€ì¡±í•˜ë©´ 0-padding, ë„˜ì¹˜ë©´ ìë¥´ê¸°)
    target_length = int(sr * duration)
    if len(y) < target_length:
        y = np.pad(y, (0, target_length - len(y)))
    else:
        y = y[:target_length]

    # Mel â†’ log-Mel ë³€í™˜
    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)
    log_mel = librosa.power_to_db(mel)

    # SpecAverage augmentation
    log_mel_aug = spec_average_augment(log_mel)

    return torch.tensor(log_mel_aug).float()

    ğŸ”§ ì ìš© ë°©ë²•
ê¸°ì¡´ ì½”ë“œì—ì„œ extract_logmel() í•¨ìˆ˜ ì „ì²´ë¥¼ ìœ„ ë‚´ìš©ìœ¼ë¡œ êµì²´í•˜ì„¸ìš”.

í˜¸ì¶œí•˜ëŠ” ìª½(ì˜ˆ: __getitem__)ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•´ë„ ì‘ë™í•©ë‹ˆë‹¤.

ğŸ“Œ ì´ë¡œ ì¸í•´ ì–»ëŠ” íš¨ê³¼
ë³€ê²½ ì „	ë³€ê²½ í›„
ì˜¤ë””ì˜¤ ê¸¸ì´ê°€ ì œê°ê° â†’ ì‹œê°„ì¶• ê¸¸ì´ ë‹¤ë¦„ â†’ ëª¨ë¸ì´ ê¸¸ì´ë¡œ í•™ìŠµ ê°€ëŠ¥ì„±	ì˜¤ë””ì˜¤ ê¸¸ì´ë¥¼ 5ì´ˆë¡œ ê³ ì • â†’ ì‹œê°„ì¶•ë„ ì¼ì • â†’ ëª¨ë¸ì´ ê¸¸ì´ë¡œ íŒë‹¨ ë¶ˆê°€
fakeëŠ” 5ì´ˆ, realì€ 10ì´ˆ â†’ ë¬´ì˜ì‹ì  bias	ë™ì¼í•œ ê¸¸ì´ì—ì„œ ë‚´ìš© ê¸°ë°˜ í•™ìŠµ ê°€ëŠ¥



