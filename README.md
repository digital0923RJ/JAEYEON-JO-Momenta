# JAEYEON-JO-Momenta

## ğŸ” Audio Deepfake Detection Models Comparison Table

| Model       | Key Technical Innovation | Reported Performance | Why It's Promising | Potential Limitations |
|-------------|---------------------------|-----------------------|----------------------|------------------------|
| **ResNet2** | - Residual learning structure<br>- Can incorporate dilated convolution & attention pooling | - EER â‰¤ **1.6%** (ADD 2022)<br>- Accuracy **98%+** | - Strong deep feature extraction<br>- Robust across varied conditions<br>- Well-suited for conversation-based detection | - High computational cost<br>- Limited ability to capture long-term context (needs Transformer integration) |
| **LCNN**    | - Lightweight CNN with Max-Feature-Map (MFM)<br>- Effective noise suppression | - EER around **3â€“5%**<br>- Real-time capable | - Excellent for **real-time detection**<br>- Suitable for edge devices<br>- Performs well in noisy environments | - Lacks deep contextual understanding when used alone<br>- Not ideal for complex conversations |
| **Wav2Vec2.0** | - Self-supervised learning<br>- Raw audio input + Transformer-based architecture | - EER â‰¤ **2.2%**<br>- Accuracy **97%+** | - Outstanding at capturing long context, emotion, and meaning<br>- Performs well with **limited labeled data**<br>- Ideal for real conversation analysis | - Large model size and high compute cost<br>- Requires optimization for real-time deployment |

---

### âœ… Summary Table 

| Model       | Accuracy | EER     | Real-time Capability | Conversational Analysis |
|-------------|----------|---------|------------------------|--------------------------|
| ResNet2     | 98%+     | â‰¤1.6%   | âš ï¸ Moderate (needs optimization) | âœ… Highly suitable |
| LCNN        | ~95%     | 3â€“5%    | âœ… Very high           | âš ï¸ Limited |
| Wav2Vec2.0  | 97%+     | â‰¤2.2%   | âŒ Needs high resources | âœ… Best performance |


## 2. ğŸ“Š Model Analysis

### ğŸ¤– Models Considered

| Model        | Key Features                                      | EER      | Real-Time | Contextual Strength |
|--------------|---------------------------------------------------|----------|------------|----------------------|
| **LCNN**     | Lightweight CNN + Max-Feature-Map                 | 3â€“5%     | âœ… High     | âš ï¸ Limited           |
| **ResNet2**  | Residual learning + dilated conv + attention pooling | â‰¤1.6% | âš ï¸ Moderate | âœ… Strong            |
| **Wav2Vec2.0** | Transformer-based, raw waveform input            | â‰¤2.2%    | âŒ No       | âœ… Excellent         |

---

### ğŸ” Why LCNN?

- âœ… Real-time capable  
- âœ… Easy to deploy on edge devices  
- âœ… Compatible with log-Mel features  
- âœ… Performs well with low-compute environments  

---

### ğŸ”§ Preprocessing Strategy

| Method        | Pros                                      | Cons                               |
|---------------|-------------------------------------------|------------------------------------|
| **MFCC**      | Compact, traditional                      | May lose fine-grained patterns     |
| **Log-Mel**   | Preserves spectro-temporal details        | Needs augmentation to generalize   |
| **SpecAverage** | Adds regularization via masking          | Requires careful tuning            |

> **Final Choice**: `log-Mel + SpecAverage`, fixed to **5-second duration**  
> â†’ Prevents length-based bias and encourages content-based learning

---

## 3. ğŸ“ˆ Performance (Ongoing)

Model training is currently in progress. Early observations suggest:

- âš¡ Fast convergence (LCNN overfits quickly â†’ needs regularization)
- ğŸ¯ Accuracy reaches 100% too early â†’ dataset still small and simple

### ğŸ” Metrics to be Evaluated:

- Accuracy  
- AUC (ROC)  
- EER (Equal Error Rate)

---

## 4. ğŸ§  Reflections

### Q1: What were the most significant challenges?

- Ensuring **model generalization**
- Removing **length-based bias**
- Balancing **practicality (speed)** vs. **performance (accuracy)**

---

### Q2: How might this approach perform in the real world?

- Requires diverse and noisy datasets to generalize well  
- LCNN is feasible for **real-time deployment**  
- Wav2Vec2.0 may need **distillation or pruning** to be practical

---

### Q3: What additional data would improve performance?

- Noisy real-world environments  
- Non-English speakers  
- Newer fake generation methods: **Vall-E, VITS, Bark**, etc.

---

### Q4: How would you deploy this?

- Export as **ONNX** or **TorchScript**  
- Apply **5-second sliding window inference**  
- Deploy in **real-time streaming pipelines** (e.g., call center audio monitoring)

---

## ğŸ“š Appendix

### ğŸ“ Dataset Info

- **Real**: LJSpeech (~13,000 clips, ~10 seconds each)  
- **Fake**: MelGAN-generated audio (~5 seconds)  
- âœ… Class-balanced cache system implemented

---

### ğŸ§ª Training Pipeline Highlights

- `extract_logmel()` ensures uniform audio duration  
- Padding applied to ensure consistent input dimensions  
- `pad_collate_fn()` used for batch-wise padding  
- Still requires **early stopping or regularization**

---

## âœ… Summary Table

| Feature                | LCNN          | ResNet2        | Wav2Vec2.0     |
|------------------------|---------------|----------------|----------------|
| Inference Speed        | âš¡ Very fast   | âš ï¸ Moderate     | ğŸ¢ Very slow    |
| Accuracy (Expected)    | ğŸ‘ ~95%        | âœ… 98%+         | âœ… 97%+         |
| EER (Expected)         | âš ï¸ 3â€“5%        | âœ… â‰¤1.6%        | âœ… â‰¤2.2%        |
| Suitable for Streaming | âœ… Yes         | âš ï¸ With tuning  | âŒ Not ideal    |

---

## ğŸš€ Future Work

- âœ… Integrate **multi-model ensemble** (LCNN + ResNet2 + distilled Wav2Vec2.0)  
- âœ… Fine-tune on **ASVspoof 2021**, **FakeAVCeleb**, or more recent datasets  
- âœ… Experiment with **attention pooling**, **SpecMix**, **SpecAugment++**  
- âœ… Build a **frontend dashboard** for live detection and visualization

---





# ğŸ§ Audio Deepfake Detection using log-Mel + SpecAverage + LCNN

> Lightweight audio deepfake detection with strong real-time capability and generalization focus.  
> Explored multiple models and preprocessing methods.  
> Final implementation uses LCNN with SpecAverage-augmented log-Mel features.

---

## ğŸ“Œ Overview

This project focuses on building an efficient and practical pipeline for detecting AI-generated human speech using lightweight CNN-based models. The goal is real-time applicability while maintaining robust detection capability.

---

## 1. âš™ï¸ Implementation Documentation

### âœ… Challenges Encountered

1. **Large Fake Dataset**
   - Slow preprocessing and loading due to dataset size
   - Limited generalization: all audio was generated
   - Real vs. fake audio had different durations (10s vs. 5s)

2. **Wav2Vec2.0 Inference Cost**
   - Too large for real-time or mobile deployment
   - Slower than lightweight CNNs like LCNN and ResNet2

3. **File Extension Issues**
   - `.wav` vs `.WAV` (case-sensitive) caused loading failures

4. **Overfitting Risk**
   - Small dataset + deep CNNs = higher chance of memorization

5. **Preprocessing Dilemma**
   - MFCC compresses too much
   - Log-Mel is better but vulnerable to overfitting
   - SpecAverage helps with augmentation but needs tuning

6. **Length-Based Bias**
   - Model might learn to classify based on input length (real=10s, fake=5s)

---

### ğŸ› ï¸ How These Were Addressed

- âœ… Added real audio: **LJSpeech** dataset
- âœ… Switched to lightweight models: **LCNN**, **ResNet2**
- âœ… Preprocessing: used **log-Mel + SpecAverage**
- âœ… Fixed-length audio: trimmed/padded all to **5 seconds**
- âœ… Created custom `extract_logmel()` with duration constraint

```python
def extract_logmel(path, sr=16000, n_mels=40, duration=5.0):
    y, _ = librosa.load(path, sr=sr)
    target_len = int(sr * duration)
    y = np.pad(y, (0, max(0, target_len - len(y))))[:target_len]
    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)
    logmel = librosa.power_to_db(mel)
    return torch.tensor(spec_average_augment(logmel)).float().unsqueeze(0)


# ğŸ§¾ Part 3: Documentation & Analysis

## 1. Implementation Documentation

### âœ… Challenges Encountered

1. **ë°ì´í„°ì…‹ì˜ í¬ê¸° ë¬¸ì œ**  
   ë‹¤ìš´ë¡œë“œí•œ fake ì˜¤ë””ì˜¤ ë°ì´í„°ì…‹ì´ ë„ˆë¬´ ì»¤ì„œ ì „ì²˜ë¦¬ì™€ ë¡œë”©ì— ì˜¤ëœ ì‹œê°„ì´ ê±¸ë ¸ê³ , ëª¨ë‘ generated audioë¼ì„œ ì¼ë°˜í™”ì— ì œí•œì´ ìˆì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
   ì‹¤ì œë¡œ ëª¨ë¸ì„ ëŒë ¤ë³¼ ì‹œê°„ì´ ë§ì´ ì—†ì—ˆìŒ. 

2. **Wav2Vec 2.0 ëª¨ë¸ì˜ ì‹¤ì‹œê°„ ì²˜ë¦¬ í•œê³„**  
   Wav2Vec2.0ì€ powerfulí•˜ê¸´ í•˜ì§€ë§Œ, í¬ê¸°ê°€ ì»¤ì„œ real-time ì ìš©ì—ëŠ” ë¶€ì í•©í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ëª¨ë°”ì¼/ê²½ëŸ‰í™”ê°€ ì¤‘ìš”í•œ í™˜ê²½ì—ì„œëŠ” ì‚¬ìš©ì´ ì–´ë ¤ì›€.

3. ë°ì´í„° ì „ì²˜ë¦¬: ê³¼ì œë¥¼ ìœ„í•œ ë…¼ë¬¸ë“¤ì—ì„œ ëŒ€ë¶€ë¶„ì˜ audio deepfake detection modelì˜ í•œê³„ì ì„ ì¼ë°˜í™”ë¼ê³  í•¨. ì¦‰,
   ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì–´ë–»ê²Œ í•´ì•¼ ì¼ë°˜í™” ì‹œ ë–¨ì–´ì§€ëŠ” ì„±ëŠ¥ì„ ìµœëŒ€í•œ ë°©ì§€í•  ìˆ˜ ìˆì„ê¹Œ ê³ ë¯¼í•¨. Noiseê°€ ì—†ëŠ” ê³ ì„±ëŠ¥ íŒŒì¼ë§ê³  noiseê°€ ìˆëŠ” íŒŒì¼, ê·¸ë¦¬ê³  ë˜ ìµœì‹ ì˜ ê¸°ìˆ ë¡œ ìƒì„±ëœ deepfake audio ë“± ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ í™•ë³´ê°€ í•„ìš”í•¨ 
   MFCC:
   Mel:
   SpecAverage:  ìœ„ì˜ 3ê°€ì§€ ì „ì²˜ë¦¬ ë°©ë²•ìœ¼ë¡œëŠ” ëª¨ë“  deepfake audio detectionì— í•œê³„ê°€ ìˆìŒ. 
 
4. ì¼ë°˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì‚¬ìš©í•˜ì§€ ì•Šê¸°. 
   ì¼ë°˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë‹ˆê¹Œ í™œì„±í™” í•¨ìˆ˜ê°€ gradientë¥¼ 0ìœ¼ë¡œ ë§Œë“¬. ì¦‰, ê¹Šì´ ìˆëŠ” ì‹ ê²½ë§ì„ ê±´ì¶•í•  ìˆ˜ê°€ ì—†ìŒ. 
---

### âœ… How Challenges Were Addressed

1. **ì‹¤ì œ(real) ì˜¤ë””ì˜¤ ë°ì´í„°ì…‹ ì¶”ê°€ í™•ë³´**  
   LJSpeech ë°ì´í„°ì…‹(`"/content/drive/MyDrive/Internship /LJSpeech-1.1/wavs"`)ì„ ì‚¬ìš©í•´ ì¼ë°˜ ìŒì„±ê³¼ ë¹„êµ ê°€ëŠ¥í•˜ë„ë¡ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.

2. **Wav2Vec 2.0 ì œê±°**  
   ì´ˆê¸°ì—ëŠ” Wav2Vec2.0ì„ transfer learning í˜•íƒœë¡œ feature extractorë¡œ ì‚¬ìš©í•˜ë ¤ í–ˆì§€ë§Œ, ëª¨ë¸ì´ ë„ˆë¬´ ì»¤ì„œ ì†ë„ì™€ ì‹¤ìš©ì„± ë©´ì—ì„œ ë¶€ì í•©í•˜ë‹¤ê³  íŒë‹¨.  
   **ì •í™•ë„ ì°¨ì´ê°€ í¬ì§€ ì•Šë‹¤ë©´, ì‹¤ìš©ì ì´ê³  ë¹ ë¥¸ ëª¨ë¸ì´ ìƒìš©í™”ì— ë” ì í•©**í•˜ë‹¤ê³  ìƒê°í•˜ì—¬ LCNNê³¼ ResNetV2ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë¹„êµ ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

3. **ë°ì´í„° ì „ì²˜ë¦¬**
   ë§ì€ ë°ì´í„° ì „ì²˜ë¦¬ ê¸°ë²•ì´ ë…¼ë¬¸ì— ìˆì—ˆë‹¤. MFCC, Log-Mel, SpecAverage. ì—¬ê¸°ì„œ ê°€ì¥ ì •í™•í•œ ì „ì²˜ë¦¬ ë°©ë²•ì„ ì°¾ì•„ì•¼ í–ˆì—ˆë‹¤.
   ì™œëƒë©´ ì´ë¯¸ Wav2Vec2.0ë¥¼ ì œê±° í–ˆê¸°ì— ê°€ëŠ¥í•œ ë§ì€ feature extractionì„ í–ˆì–´ì•¼ í–ˆë‹¤.  
   ê·¸ë¦¬ê³  
   - MFCC
   - Log-Mel
   - SpecAverage
  
4. ë”¥ëŸ¬ë‹ ì‚¬ìš©(light version)
   ê¹Šì´ ìˆëŠ” ì‹ ê²½ë§ì´ ìˆì„ ìˆ˜ë¡ í•™ìŠµì„ ë” ì˜í•  ìˆ˜ ìˆê¸°ì— ì´ë¥¼ ì˜ í•  ìˆ˜ ìˆê¸°ì— LCNN ì‚¬ìš©í•¨.
   
---

## Part 3: Documentation & Analysis

1. Document your implementation process, including:
    - Any challenges encountered:
   1. ë‹¤ìš´ë¡œë“œí•œ fake ì˜¤ë””ì˜¤ ë°ì´í„°ì…‹ì´ ë„ˆë¬´ ì»¤ì„œ ì „ì²˜ë¦¬ì™€ ë¡œë”©ì— ì˜¤ëœ ì‹œê°„ì´ ê±¸ë ¸ê³ , ëª¨ë‘ generated audioë¼ì„œ ì¼ë°˜í™”ì— ì œí•œì´ ìˆì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
   2. Wav2Vec2.0ì€ powerfulí•˜ê¸´ í•˜ì§€ë§Œ, í¬ê¸°ê°€ ì»¤ì„œ real-time ì ìš©ì—ëŠ” ë¶€ì í•©í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ëª¨ë°”ì¼/ê²½ëŸ‰í™”ê°€ ì¤‘ìš”í•œ í™˜ê²½ì—ì„œëŠ” ì‚¬ìš©ì´ ì–´ë ¤ì›€.
  
    - How you addressed these challenges:
    - 1. ì‹¤ì œ ë°ì´í„° ì…‹ì„ êµ¬í–ˆë‹¤ - ("/content/drive/MyDrive/Internship /LJSpeech-1.1/wavs")ì½”ë“œì— ìˆìŒ),
    - 2. ì´ˆê¸°ì—ëŠ” Wav2Vec2.0ì„ transfer learning í˜•íƒœë¡œ feature extractorë¡œ ì‚¬ìš©í•˜ë ¤ í–ˆì§€ë§Œ, ëª¨ë¸ì´ ë„ˆë¬´ ì»¤ì„œ ì†ë„ì™€ ì‹¤ìš©ì„± ë©´ì—ì„œ ë¶€ì í•©í•˜ë‹¤ê³  íŒë‹¨.
ì •í™•ë„ ì°¨ì´ê°€ í¬ì§€ ì•Šë‹¤ë©´, ì‹¤ìš©ì ì´ê³  ë¹ ë¥¸ ëª¨ë¸ì´ ìƒìš©í™”ì— ë” ì í•©í•˜ë‹¤ê³  ìƒê°í•˜ì—¬ LCNNê³¼ ResNetV2ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë¹„êµ ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤
   - LCNN ì…ë ¥í•  ë•Œ, featureì˜ í¬ê¸°ê°€ ë‹¤ë¦„.
   - ìƒˆë¡œ ì…ë ¥í•œ ë°ì´í„°ì…‹ ì˜¤ë””ì˜¤ ê¸¸ì´ê°€ ê¸¸ì—ˆìŒ.  
    - Assumptions made:  
  
3. Include an analysis section that addresses:
    - Why you selected this particular model for implementation: ê²°ê³¼ë¥¼ ë³´ë‹ˆ ì•™ìƒë¸”ì´ ì¢‹ë‹¤ëŠ” íš¨ê³¼ë¥¼ ë´¤ë‹¤
    - How the model works (high-level technical explanation): SpecAverage ì „ì²˜ë¦¬í•˜ê³  logMELì„ í•¨. ì›ë˜ëŠ” MFCCë¥¼ í•˜ë ¤ê³  í–ˆëŠ”ë°, ê·¼ë° ë„ˆ ê°™ìœ¼ë©´ ì™œ SpecAverge+logMelì„ ì„ íƒí•´? over MFCC? 
    - Performance results on your chosen dataset: ì•„ì§ ëª¨ë¸ ëŒë¦¬ëŠ” ì¤‘ 
    - Observed strengths and weaknesses: 
    - Suggestions for future improvements: 

4. Reflection questions to address:
    1. What were the most significant challenges in implementing this model? 
    2. How might this approach perform in real-world conditions vs. research datasets?
    3. What additional data or resources would improve performance?
    4. How would you approach deploying this model in a production environment?
  
   í™•ì¥ì ã……ã…‚....wav. WAV ì°¨ì´ê°€ ìˆì„ì§€...

   ğŸ¯ ë¬¸ì œ ë‹¤ì‹œ ì •ë¦¬:
real ì˜¤ë””ì˜¤ëŠ” ì•½ 10ì´ˆ, fake ì˜¤ë””ì˜¤ëŠ” ì•½ 5ì´ˆ
â†’ ëª¨ë¸ì´ "ê¸¸ì´ ì°¨ì´"ë§Œìœ¼ë¡œ ì§„ì§œ/ê°€ì§œë¥¼ ë¶„ë¥˜í•  ìœ„í—˜
â†’ ì¼ë°˜í™” ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥

def extract_logmel(path, sr=16000, n_mels=N_MELS):
    y, _ = librosa.load(path, sr=sr)
    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)
    ...
    ë”°ë¼ì„œ log-Melë„ ì‹œê°„ ì¶•(t) ê¸¸ì´ê°€ ì„œë¡œ ë‹¬ë¼ì§‘ë‹ˆë‹¤.
    íŒ¨ë”©: ì§§ì€ ì˜¤ë””ì˜¤ íŒŒì¼ ë’¤ì— 0 ê°’ì„ ì±„ì›Œ ê¸¸ì´ë¥¼ ë§ì¶”ëŠ” ë°©ë²•
    ë¬¸ì œ ìˆìŒ: librosa.load()ëŠ” ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë”°ë¼ ê·¸ëŒ€ë¡œ ë¡œë”©í•©ë‹ˆë‹¤.

ì¦‰, realì€ 10ì´ˆ â†’ 160,000 samples, fakeëŠ” 5ì´ˆ â†’ 80,000 samples

ë”°ë¼ì„œ log-Melë„ ì‹œê°„ ì¶•(t) ê¸¸ì´ê°€ ì„œë¡œ ë‹¬ë¼ì§‘ë‹ˆë‹¤.

def pad_collate_fn(batch):
    features, labels = zip(*batch)
    max_len = max([f.shape[-1] for f in features])
    padded = [torch.nn.functional.pad(f, (0, max_len - f.shape[-1])) for f in features]
    return torch.stack(padded), torch.tensor(labels).float()
ë¶€ë¶„ì ìœ¼ë¡œ í•´ê²°ë¨: ë°°ì¹˜ ë‹¨ìœ„ì—ì„œëŠ” spectrogramì˜ ê¸¸ì´ë¥¼ íŒ¨ë”©ìœ¼ë¡œ ë§ì¶”ê³  ìˆìŒ

í•˜ì§€ë§Œ ì›ë˜ ê¸¸ì´ì— ë”°ë¥¸ ì •ë³´ëŠ” ì—¬ì „íˆ featureì— ë‚¨ì•„ ìˆìŒ

ì¦‰, ëª¨ë¸ì´ ì—¬ì „íˆ **"ì§§ì€ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì´ë©´ fake, ê¸¸ë©´ real"**ì´ë¼ëŠ” íŒíŠ¸ë¥¼ í•™ìŠµí•  ê°€ëŠ¥ì„± ìˆìŒ
â†’ í•™ìŠµì´ ê¸¸ì´ì— biasë  ìˆ˜ ìˆê³ , ì´ëŠ” generalizationì— ì·¨ì•½ì ì´ ë©ë‹ˆë‹¤.


ëª¨ë“  ì˜¤ë””ì˜¤ë¥¼ 5ì´ˆë¡œ ì˜ë¼ì„œ ê³ ì • â†’ ê¸¸ì´ ì •ë³´ê°€ ëª¨ë¸ì— ë°˜ì˜ë˜ì§€ ì•ŠìŒ

ì§„ì§œ/ê°€ì§œ ëª¨ë‘ ê°™ì€ ê¸¸ì´ë¡œ ë¹„êµë˜ë¯€ë¡œ ê¸¸ì´ì— ì˜ì¡´í•œ ë¶„ë¥˜ ê°€ëŠ¥ì„± ì°¨ë‹¨
âœ… ê²°ë¡ 
í•­ëª©	í˜„ì¬ ì½”ë“œ	í•´ê²° ì—¬ë¶€
ì˜¤ë””ì˜¤ ê¸¸ì´ í†µì¼	âŒ (librosa.load()ë¡œ ì›ë³¸ ê¸¸ì´ ìœ ì§€)	í•´ê²° ì•ˆ ë¨
ê¸¸ì´ ë§ì¶¤ ì²˜ë¦¬	â­• (pad_collate_fnìœ¼ë¡œ íŒ¨ë”©)	ì¼ë¶€ í•´ê²° (ëª¨ì–‘ ë§ì¶¤ë§Œ)
ê¸¸ì´ bias ì œê±°	âŒ ëª¨ë¸ì´ ì—¬ì „íˆ ê¸¸ì´ ì •ë³´ì— ì ‘ê·¼ ê°€ëŠ¥	ìˆ˜ì • í•„ìš”


âœ… ìˆ˜ì •ëœ extract_logmel() í•¨ìˆ˜
python
ë³µì‚¬
í¸ì§‘
def extract_logmel(path, sr=16000, n_mels=N_MELS, duration=5.0):
    y, _ = librosa.load(path, sr=sr)

    # ê¸¸ì´ë¥¼ 5ì´ˆë¡œ ê³ ì • (ë¶€ì¡±í•˜ë©´ 0-padding, ë„˜ì¹˜ë©´ ìë¥´ê¸°)
    target_length = int(sr * duration)
    if len(y) < target_length:
        y = np.pad(y, (0, target_length - len(y)))
    else:
        y = y[:target_length]

    # Mel â†’ log-Mel ë³€í™˜
    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)
    log_mel = librosa.power_to_db(mel)

    # SpecAverage augmentation
    log_mel_aug = spec_average_augment(log_mel)

    return torch.tensor(log_mel_aug).float()

    ğŸ”§ ì ìš© ë°©ë²•
ê¸°ì¡´ ì½”ë“œì—ì„œ extract_logmel() í•¨ìˆ˜ ì „ì²´ë¥¼ ìœ„ ë‚´ìš©ìœ¼ë¡œ êµì²´í•˜ì„¸ìš”.

í˜¸ì¶œí•˜ëŠ” ìª½(ì˜ˆ: __getitem__)ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•´ë„ ì‘ë™í•©ë‹ˆë‹¤.

ğŸ“Œ ì´ë¡œ ì¸í•´ ì–»ëŠ” íš¨ê³¼
ë³€ê²½ ì „	ë³€ê²½ í›„
ì˜¤ë””ì˜¤ ê¸¸ì´ê°€ ì œê°ê° â†’ ì‹œê°„ì¶• ê¸¸ì´ ë‹¤ë¦„ â†’ ëª¨ë¸ì´ ê¸¸ì´ë¡œ í•™ìŠµ ê°€ëŠ¥ì„±	ì˜¤ë””ì˜¤ ê¸¸ì´ë¥¼ 5ì´ˆë¡œ ê³ ì • â†’ ì‹œê°„ì¶•ë„ ì¼ì • â†’ ëª¨ë¸ì´ ê¸¸ì´ë¡œ íŒë‹¨ ë¶ˆê°€
fakeëŠ” 5ì´ˆ, realì€ 10ì´ˆ â†’ ë¬´ì˜ì‹ì  bias	ë™ì¼í•œ ê¸¸ì´ì—ì„œ ë‚´ìš© ê¸°ë°˜ í•™ìŠµ ê°€ëŠ¥



